{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Tensorflow\n",
    "\n",
    "I will teach myself how to use tensorflow and will document that process in this notebook. I am trying to go thru as much as I can of the official guide: https://www.tensorflow.org/guide. Nothing here is original, it's just going thru the guide and explaining what's going on in my own words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and Variables\n",
    "\n",
    "Tensors are a fancy name for n-dimensional arrays. The # of dimensions is called the rank. A 1-dimensional array, like [5], is a tensor with rank 1. A tensor [[2,3],[4,1]] has rank 2 and dimensions (2,2). \n",
    "\n",
    "The main types of Tensors are:  \n",
    "tf.Variable  \n",
    "tf.constant  \n",
    "tf.placeholder  \n",
    "tf.SparseTensor  \n",
    "\n",
    "Except for tf.Variable, they are all immutable within the execution of a session. Elements of a Tensor must all be the same datatype and can be quite varied. tf.Variables can exist outside the context of a single session.run call, which is not true of other tf.Tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable([[5.0],[2.0],[3]])\n",
    "b = tf.Variable([[5,1],[2,1],[3,3]], tf.int32)\n",
    "c = tf.Variable(['Hello','World'], tf.string)\n",
    "print(a,b,c)\n",
    "print(tf.rank(a), tf.rank(b), tf.rank(c))\n",
    "print(b[2])\n",
    "print(b[2,0])\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables get added to two collections:  \n",
    "tf.GraphKeys.GLOBAL_VARIABLES - variables that can be shared across devices  \n",
    "tf.GraphKeys.TRAINABLE_VARIABLES - variables for which Tensorflow will calculate gradients  \n",
    "Collections are a way to group variables that may not be connected in the graph. You can make your own collection by just adding a variable to it. You can also place variables on particular devices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proper way to create a variable\n",
    "if not my_var:\n",
    "    my_var = tf.get_variable(\"my_var\", [1,2,3])\n",
    "print(my_var)\n",
    "\n",
    "#add it to a collection\n",
    "tf.add_to_collection(\"my_collection\", my_var)\n",
    "print(tf.get_collection(\"my_collection\"))\n",
    "\n",
    "#adding my_var to device\n",
    "with tf.device(\"/device:CPU:0\"):\n",
    "    if not v:\n",
    "        v = tf.get_variable(\"v\", [2,2])\n",
    "    \n",
    "#prints available devices\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "#convert different objects to tensors\n",
    "np_array = np.zeros((40,3))\n",
    "some_list = [[1,2,3,4],[2,5,1,1]]\n",
    "np_tensor = tf.convert_to_tensor(np_array)\n",
    "list_tensor = tf.convert_to_tensor(some_list)\n",
    "print(np_tensor)\n",
    "print(list_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting feature here is that you can specify the scope in which variables are created. For example, variables created within function do *not* just live within the scope of the function. This allows you to reuse those variables elsewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_operation(x):\n",
    "    w = tf.get_variable(\"w\", [2,2], initializer=tf.random_normal_initializer())\n",
    "    return w + x\n",
    "\n",
    "x = tf.random_normal([2,2])\n",
    "y = some_operation(x)\n",
    "try:\n",
    "    z = some_operation(y)\n",
    "except ValueError:\n",
    "    print('did not like this')\n",
    "    \n",
    "with tf.variable_scope(\"one\"):\n",
    "    y = some_operation(x)\n",
    "with tf.variable_scope(\"two\"):\n",
    "    z = some_operation(y)\n",
    "    print('function ran on a different scope, this it liked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs and Sessions\n",
    "A tensorflow program consists of a computational graph and a session, which runs the graph. The word graph is meant in the discrete math graph theory sense. A graph consists of nodes and vertices. Vertices are tensors, i.e. arrays, which flow thru nodes, which are operations. So arrays pass thru nodes where they are operated upon, and the output tensor may flow elsewhere to be further processed. Below we define a simple graph with 1-dimensional tensors (constants), and a node that adds them together.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(3.0, dtype=tf.float32)\n",
    "b = tf.constant(4.0) # also tf.float32 implicitly\n",
    "total = a + b\n",
    "print(a)\n",
    "print(b)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational graph has just been defined *but not* run. That is why total doesn't say 7.  \n",
    "\n",
    "OK, now we define a session. The session takes three optional arguments:  \n",
    "1) target - default uses devices only on local machine, whilst grpc:// allows you to use a remote TensorFlow server  \n",
    "2) graph - it operates on the default graph but you can specify which graph you want it to operate on\n",
    "3) config - bunch of states which alter the way the graph is computed\n",
    "\n",
    "The tf.Session.run method is how you compute parts of the graph. You can put any number of nodes or edges you want to compute. Does not have to be the whole graph.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(total))\n",
    "print(sess.run({'ab':(a,b), 'total':total, 'sandwhich': (b,total)}))\n",
    "\n",
    "\n",
    "vec = tf.random_uniform(shape=(3,))\n",
    "out1 = vec + 1\n",
    "out2 = vec + 2\n",
    "#different values between the first 2\n",
    "print(sess.run(vec))\n",
    "print(sess.run(vec))\n",
    "#this time, vec is run once as part of the same graph\n",
    "print(sess.run((out1, out2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is kind of like a function z(x,y) = x+y\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = x + y\n",
    "\n",
    "print(sess.run(z, feed_dict={x:3, y:5}))\n",
    "print(sess.run(z, feed_dict={x:[1,2,1,1,1], y:[2,-1,4,-5,4]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "datasets are preferred over placeholders. basically you take a \"dataset\" and iterate over some dimension\n",
    "'''\n",
    "my_data = [\n",
    "    [0, 1,],\n",
    "    [2, 3,],\n",
    "    [4, 5,],\n",
    "    [6, 7,],\n",
    "]\n",
    "slices = tf.data.Dataset.from_tensor_slices(my_data)\n",
    "print(slices)\n",
    "next_item = slices.make_one_shot_iterator().get_next()\n",
    "print(next_item)\n",
    "while True:\n",
    "  try:\n",
    "    print(sess.run(next_item))\n",
    "  except tf.errors.OutOfRangeError:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. I get how we are making the slices and iterate over them with a generator-like way. At the end it throws an OutOfRange exception and that's your cue to break. I don't like these, the methods have ugly names, haha. It seems unnecessarily complicated but I'm sure I'll realize later why they did it this way. Let's just carry on.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "Layers combine both variables and the operations that act on them. In a neural network, a densely-connected layer will perform a weighted sum of the inputs and then apply a thresholding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Vertex in graph\n",
    "x = tf.placeholder(tf.float32, shape=(None,3))\n",
    "linear_model = tf.layers.Dense(units=2)\n",
    "#default activation is linear, i.e. identity\n",
    "print(linear_model.activation)\n",
    "\n",
    "#the internal matrix is instantiated once the input is known\n",
    "print('before putting input in:', linear_model.weights)\n",
    "y = linear_model(x)\n",
    "print('after putting input in:', linear_model.weights)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "print(sess.run(y, {x:[[1,2,3],[4,5,6]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lets define a simple linear regression with the building blocks we just discovered\n",
    "'''\n",
    "\n",
    "#define some data\n",
    "x = tf.constant([[1],[2],[3],[4]], dtype=tf.float32)\n",
    "y_true = tf.constant([[0],[-2],[-2],[-3]], dtype=tf.float32)\n",
    "\n",
    "#define model\n",
    "linear_model = tf.layers.Dense(units=1)\n",
    "y_pred = linear_model(x)\n",
    "\n",
    "#run graph\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print(sess.run(y_pred))\n",
    "\n",
    "#define loss\n",
    "loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)\n",
    "print('mse: ', sess.run(loss))\n",
    "\n",
    "#define optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "for i in range(100):\n",
    "    _, loss_val = sess.run((train, loss))\n",
    "    \n",
    "print('after training', sess.run(y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "We will visualize the graph using TensorBoard. This will be useful for the work we may do with Kanaka. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('/Users/pablomartin/python/tensorflow/graphs/')\n",
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code above wrote a file to given directory. It must be opened with the following command:  \n",
    "\n",
    "python -m tensorboard.main --logdir=tensorflow/graphs/    \n",
    "\n",
    "Then we can view the graph on a browser. There are ways to specify which part of the graph to show or not, but until we know what we want, let's just leave it at that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Keras \n",
    "I am following the tensorflow guide on keras here. First we build a multi-layer perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential()\n",
    "#very similar to regular keras\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "#by default, no activation is applied, so you must specify something\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "#softmax layers to categorize into one of 10 categories\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "#again very similar, you have to specify learning rate on optimizer though\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1000\n",
      "1000/1000 [==============================] - 0s 394us/step - loss: 11.6982 - acc: 0.1090 - val_loss: 11.3536 - val_acc: 0.0700\n",
      "Epoch 2/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.6676 - acc: 0.0940 - val_loss: 11.3480 - val_acc: 0.0700\n",
      "Epoch 3/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.6603 - acc: 0.1160 - val_loss: 11.3442 - val_acc: 0.0700\n",
      "Epoch 4/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.6548 - acc: 0.1200 - val_loss: 11.3435 - val_acc: 0.0300\n",
      "Epoch 5/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.6506 - acc: 0.1310 - val_loss: 11.3453 - val_acc: 0.0700\n",
      "Epoch 6/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.6454 - acc: 0.1460 - val_loss: 11.3476 - val_acc: 0.0700\n",
      "Epoch 7/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.6415 - acc: 0.1450 - val_loss: 11.3401 - val_acc: 0.0600\n",
      "Epoch 8/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.6369 - acc: 0.1450 - val_loss: 11.3464 - val_acc: 0.0500\n",
      "Epoch 9/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.6335 - acc: 0.1400 - val_loss: 11.3454 - val_acc: 0.0800\n",
      "Epoch 10/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.6294 - acc: 0.1430 - val_loss: 11.3498 - val_acc: 0.1000\n",
      "Epoch 11/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.6235 - acc: 0.1590 - val_loss: 11.3487 - val_acc: 0.0700\n",
      "Epoch 12/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.6208 - acc: 0.1540 - val_loss: 11.3534 - val_acc: 0.0500\n",
      "Epoch 13/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.6159 - acc: 0.1710 - val_loss: 11.3470 - val_acc: 0.0900\n",
      "Epoch 14/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.6109 - acc: 0.1910 - val_loss: 11.3554 - val_acc: 0.0700\n",
      "Epoch 15/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.6085 - acc: 0.1820 - val_loss: 11.3509 - val_acc: 0.0800\n",
      "Epoch 16/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 11.6039 - acc: 0.1770 - val_loss: 11.3563 - val_acc: 0.0800\n",
      "Epoch 17/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.5987 - acc: 0.1900 - val_loss: 11.3516 - val_acc: 0.1000\n",
      "Epoch 18/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.5951 - acc: 0.1930 - val_loss: 11.3614 - val_acc: 0.0800\n",
      "Epoch 19/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 11.5896 - acc: 0.1980 - val_loss: 11.3666 - val_acc: 0.0800\n",
      "Epoch 20/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.5847 - acc: 0.2010 - val_loss: 11.3657 - val_acc: 0.0800\n",
      "Epoch 21/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 11.5632 - acc: 0.18 - 0s 50us/step - loss: 11.5793 - acc: 0.2160 - val_loss: 11.3728 - val_acc: 0.1100\n",
      "Epoch 22/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 11.5732 - acc: 0.2130 - val_loss: 11.3818 - val_acc: 0.0800\n",
      "Epoch 23/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.5678 - acc: 0.2060 - val_loss: 11.3777 - val_acc: 0.1000\n",
      "Epoch 24/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 11.5661 - acc: 0.2250 - val_loss: 11.3773 - val_acc: 0.1000\n",
      "Epoch 25/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 11.5591 - acc: 0.2280 - val_loss: 11.3814 - val_acc: 0.0900\n",
      "Epoch 26/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 11.5563 - acc: 0.2150 - val_loss: 11.3791 - val_acc: 0.1200\n",
      "Epoch 27/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.5497 - acc: 0.2390 - val_loss: 11.3883 - val_acc: 0.1000\n",
      "Epoch 28/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 11.5467 - acc: 0.2240 - val_loss: 11.3852 - val_acc: 0.1400\n",
      "Epoch 29/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 11.5400 - acc: 0.2320 - val_loss: 11.3884 - val_acc: 0.1400\n",
      "Epoch 30/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.5343 - acc: 0.2230 - val_loss: 11.3980 - val_acc: 0.1200\n",
      "Epoch 31/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 11.5309 - acc: 0.2290 - val_loss: 11.3947 - val_acc: 0.1100\n",
      "Epoch 32/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 11.5261 - acc: 0.2150 - val_loss: 11.3990 - val_acc: 0.1400\n",
      "Epoch 33/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.5200 - acc: 0.2380 - val_loss: 11.3969 - val_acc: 0.0900\n",
      "Epoch 34/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.5202 - acc: 0.2280 - val_loss: 11.3937 - val_acc: 0.1100\n",
      "Epoch 35/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.5109 - acc: 0.2370 - val_loss: 11.4095 - val_acc: 0.1500\n",
      "Epoch 36/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.5061 - acc: 0.2410 - val_loss: 11.3988 - val_acc: 0.1300\n",
      "Epoch 37/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.5031 - acc: 0.2400 - val_loss: 11.4129 - val_acc: 0.1400\n",
      "Epoch 38/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.4972 - acc: 0.2470 - val_loss: 11.4114 - val_acc: 0.1300\n",
      "Epoch 39/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 11.4922 - acc: 0.2410 - val_loss: 11.4135 - val_acc: 0.1700\n",
      "Epoch 40/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 11.4845 - acc: 0.2500 - val_loss: 11.4144 - val_acc: 0.1700\n",
      "Epoch 41/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.4829 - acc: 0.2540 - val_loss: 11.4219 - val_acc: 0.1600\n",
      "Epoch 42/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.4798 - acc: 0.2550 - val_loss: 11.4272 - val_acc: 0.1600\n",
      "Epoch 43/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.4750 - acc: 0.2580 - val_loss: 11.4244 - val_acc: 0.1500\n",
      "Epoch 44/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.4689 - acc: 0.2780 - val_loss: 11.4350 - val_acc: 0.1600\n",
      "Epoch 45/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.4657 - acc: 0.2510 - val_loss: 11.4256 - val_acc: 0.1900\n",
      "Epoch 46/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.4635 - acc: 0.2700 - val_loss: 11.4379 - val_acc: 0.1400\n",
      "Epoch 47/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.4575 - acc: 0.2800 - val_loss: 11.4545 - val_acc: 0.1400\n",
      "Epoch 48/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.4545 - acc: 0.2640 - val_loss: 11.4472 - val_acc: 0.1600\n",
      "Epoch 49/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.4481 - acc: 0.2810 - val_loss: 11.4388 - val_acc: 0.1800\n",
      "Epoch 50/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.4430 - acc: 0.2750 - val_loss: 11.4499 - val_acc: 0.1700\n",
      "Epoch 51/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.4418 - acc: 0.2780 - val_loss: 11.4580 - val_acc: 0.2300\n",
      "Epoch 52/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.4353 - acc: 0.2800 - val_loss: 11.4539 - val_acc: 0.1800\n",
      "Epoch 53/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.4340 - acc: 0.2720 - val_loss: 11.4686 - val_acc: 0.1800\n",
      "Epoch 54/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.4294 - acc: 0.2760 - val_loss: 11.4557 - val_acc: 0.1700\n",
      "Epoch 55/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.4271 - acc: 0.2740 - val_loss: 11.4802 - val_acc: 0.1700\n",
      "Epoch 56/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.4251 - acc: 0.2950 - val_loss: 11.4768 - val_acc: 0.2000\n",
      "Epoch 57/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.4193 - acc: 0.2980 - val_loss: 11.4669 - val_acc: 0.1700\n",
      "Epoch 58/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.4149 - acc: 0.2840 - val_loss: 11.4780 - val_acc: 0.1800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.4108 - acc: 0.3020 - val_loss: 11.4698 - val_acc: 0.1900\n",
      "Epoch 60/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.4088 - acc: 0.3030 - val_loss: 11.4790 - val_acc: 0.1900\n",
      "Epoch 61/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.4022 - acc: 0.2930 - val_loss: 11.4768 - val_acc: 0.1800\n",
      "Epoch 62/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.4003 - acc: 0.3090 - val_loss: 11.4902 - val_acc: 0.2100\n",
      "Epoch 63/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.3939 - acc: 0.3190 - val_loss: 11.4968 - val_acc: 0.2000\n",
      "Epoch 64/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.3957 - acc: 0.2990 - val_loss: 11.4914 - val_acc: 0.1800\n",
      "Epoch 65/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.3893 - acc: 0.3060 - val_loss: 11.4930 - val_acc: 0.1800\n",
      "Epoch 66/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.3850 - acc: 0.3110 - val_loss: 11.4961 - val_acc: 0.1800\n",
      "Epoch 67/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.3850 - acc: 0.3010 - val_loss: 11.4975 - val_acc: 0.1900\n",
      "Epoch 68/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.3800 - acc: 0.3070 - val_loss: 11.5083 - val_acc: 0.1900\n",
      "Epoch 69/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.3778 - acc: 0.3210 - val_loss: 11.5317 - val_acc: 0.1800\n",
      "Epoch 70/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.3750 - acc: 0.3110 - val_loss: 11.5146 - val_acc: 0.1800\n",
      "Epoch 71/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.3711 - acc: 0.3140 - val_loss: 11.5249 - val_acc: 0.1900\n",
      "Epoch 72/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.3706 - acc: 0.3110 - val_loss: 11.5205 - val_acc: 0.1700\n",
      "Epoch 73/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.3654 - acc: 0.3150 - val_loss: 11.5251 - val_acc: 0.1900\n",
      "Epoch 74/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.3608 - acc: 0.3110 - val_loss: 11.5273 - val_acc: 0.1800\n",
      "Epoch 75/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.3590 - acc: 0.3210 - val_loss: 11.5225 - val_acc: 0.1700\n",
      "Epoch 76/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.3577 - acc: 0.3240 - val_loss: 11.5349 - val_acc: 0.1800\n",
      "Epoch 77/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.3537 - acc: 0.3160 - val_loss: 11.5513 - val_acc: 0.1700\n",
      "Epoch 78/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.3528 - acc: 0.3160 - val_loss: 11.5465 - val_acc: 0.1700\n",
      "Epoch 79/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.3485 - acc: 0.3250 - val_loss: 11.5463 - val_acc: 0.1900\n",
      "Epoch 80/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.3465 - acc: 0.3150 - val_loss: 11.5468 - val_acc: 0.1600\n",
      "Epoch 81/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.3420 - acc: 0.3100 - val_loss: 11.5545 - val_acc: 0.1600\n",
      "Epoch 82/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.3420 - acc: 0.3130 - val_loss: 11.5453 - val_acc: 0.1700\n",
      "Epoch 83/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.3400 - acc: 0.3190 - val_loss: 11.5614 - val_acc: 0.1700\n",
      "Epoch 84/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.3354 - acc: 0.3250 - val_loss: 11.5578 - val_acc: 0.1700\n",
      "Epoch 85/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.3322 - acc: 0.3310 - val_loss: 11.5663 - val_acc: 0.1600\n",
      "Epoch 86/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.3306 - acc: 0.3280 - val_loss: 11.5700 - val_acc: 0.1800\n",
      "Epoch 87/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.3286 - acc: 0.3270 - val_loss: 11.5924 - val_acc: 0.1600\n",
      "Epoch 88/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.3261 - acc: 0.3340 - val_loss: 11.5810 - val_acc: 0.1700\n",
      "Epoch 89/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.3243 - acc: 0.3310 - val_loss: 11.5732 - val_acc: 0.1700\n",
      "Epoch 90/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.3198 - acc: 0.3230 - val_loss: 11.5761 - val_acc: 0.1700\n",
      "Epoch 91/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.3187 - acc: 0.3090 - val_loss: 11.5785 - val_acc: 0.1600\n",
      "Epoch 92/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.3159 - acc: 0.3380 - val_loss: 11.5978 - val_acc: 0.1600\n",
      "Epoch 93/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.3131 - acc: 0.3380 - val_loss: 11.5939 - val_acc: 0.1400\n",
      "Epoch 94/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.3119 - acc: 0.3440 - val_loss: 11.5883 - val_acc: 0.1700\n",
      "Epoch 95/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.3126 - acc: 0.3380 - val_loss: 11.5901 - val_acc: 0.1600\n",
      "Epoch 96/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.3076 - acc: 0.3430 - val_loss: 11.5971 - val_acc: 0.1500\n",
      "Epoch 97/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.3047 - acc: 0.3560 - val_loss: 11.6121 - val_acc: 0.1500\n",
      "Epoch 98/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.3026 - acc: 0.3410 - val_loss: 11.5979 - val_acc: 0.1800\n",
      "Epoch 99/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.3009 - acc: 0.3330 - val_loss: 11.6176 - val_acc: 0.1300\n",
      "Epoch 100/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.2995 - acc: 0.3450 - val_loss: 11.6046 - val_acc: 0.1500\n",
      "Epoch 101/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2977 - acc: 0.3360 - val_loss: 11.6132 - val_acc: 0.1600\n",
      "Epoch 102/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2942 - acc: 0.3520 - val_loss: 11.6129 - val_acc: 0.1500\n",
      "Epoch 103/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2930 - acc: 0.3500 - val_loss: 11.6210 - val_acc: 0.1300\n",
      "Epoch 104/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.2926 - acc: 0.3550 - val_loss: 11.6191 - val_acc: 0.1400\n",
      "Epoch 105/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 11.2870 - acc: 0.3530 - val_loss: 11.6414 - val_acc: 0.1400\n",
      "Epoch 106/1000\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 11.2863 - acc: 0.3520 - val_loss: 11.6196 - val_acc: 0.1500\n",
      "Epoch 107/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.2865 - acc: 0.3420 - val_loss: 11.6333 - val_acc: 0.1500\n",
      "Epoch 108/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 11.2832 - acc: 0.3500 - val_loss: 11.6255 - val_acc: 0.1400\n",
      "Epoch 109/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 11.2797 - acc: 0.3620 - val_loss: 11.6394 - val_acc: 0.1400\n",
      "Epoch 110/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.2794 - acc: 0.3620 - val_loss: 11.6607 - val_acc: 0.1400\n",
      "Epoch 111/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 11.2781 - acc: 0.3530 - val_loss: 11.6425 - val_acc: 0.1300\n",
      "Epoch 112/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 11.2760 - acc: 0.3650 - val_loss: 11.6612 - val_acc: 0.1300\n",
      "Epoch 113/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.2755 - acc: 0.3550 - val_loss: 11.6549 - val_acc: 0.1500\n",
      "Epoch 114/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 11.2700 - acc: 0.3630 - val_loss: 11.6506 - val_acc: 0.1300\n",
      "Epoch 115/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 11.2689 - acc: 0.3580 - val_loss: 11.6559 - val_acc: 0.1500\n",
      "Epoch 116/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.2669 - acc: 0.3720 - val_loss: 11.6725 - val_acc: 0.1300\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 45us/step - loss: 11.2660 - acc: 0.3580 - val_loss: 11.6786 - val_acc: 0.1300\n",
      "Epoch 118/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 11.2692 - acc: 0.3610 - val_loss: 11.6742 - val_acc: 0.1500\n",
      "Epoch 119/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 11.2639 - acc: 0.3620 - val_loss: 11.6719 - val_acc: 0.1300\n",
      "Epoch 120/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.2607 - acc: 0.3750 - val_loss: 11.6684 - val_acc: 0.1300\n",
      "Epoch 121/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2590 - acc: 0.3720 - val_loss: 11.6817 - val_acc: 0.1100\n",
      "Epoch 122/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2615 - acc: 0.3620 - val_loss: 11.6696 - val_acc: 0.1500\n",
      "Epoch 123/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2570 - acc: 0.3870 - val_loss: 11.6984 - val_acc: 0.1200\n",
      "Epoch 124/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2542 - acc: 0.3680 - val_loss: 11.6909 - val_acc: 0.1400\n",
      "Epoch 125/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2534 - acc: 0.3680 - val_loss: 11.6806 - val_acc: 0.1200\n",
      "Epoch 126/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2540 - acc: 0.3750 - val_loss: 11.6880 - val_acc: 0.1200\n",
      "Epoch 127/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2513 - acc: 0.3740 - val_loss: 11.7037 - val_acc: 0.1500\n",
      "Epoch 128/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2497 - acc: 0.3660 - val_loss: 11.7008 - val_acc: 0.1100\n",
      "Epoch 129/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2477 - acc: 0.3750 - val_loss: 11.6948 - val_acc: 0.1200\n",
      "Epoch 130/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2468 - acc: 0.3950 - val_loss: 11.7236 - val_acc: 0.1200\n",
      "Epoch 131/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.2473 - acc: 0.3800 - val_loss: 11.7176 - val_acc: 0.1200\n",
      "Epoch 132/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2439 - acc: 0.3700 - val_loss: 11.7103 - val_acc: 0.1200\n",
      "Epoch 133/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2427 - acc: 0.3800 - val_loss: 11.7198 - val_acc: 0.1400\n",
      "Epoch 134/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2388 - acc: 0.3880 - val_loss: 11.7204 - val_acc: 0.1100\n",
      "Epoch 135/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2376 - acc: 0.3820 - val_loss: 11.7131 - val_acc: 0.1000\n",
      "Epoch 136/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2388 - acc: 0.3730 - val_loss: 11.7199 - val_acc: 0.1400\n",
      "Epoch 137/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2374 - acc: 0.3850 - val_loss: 11.7335 - val_acc: 0.1200\n",
      "Epoch 138/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2378 - acc: 0.3770 - val_loss: 11.7332 - val_acc: 0.1300\n",
      "Epoch 139/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2368 - acc: 0.3830 - val_loss: 11.7225 - val_acc: 0.1200\n",
      "Epoch 140/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2283 - acc: 0.3860 - val_loss: 11.7435 - val_acc: 0.1400\n",
      "Epoch 141/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2316 - acc: 0.3810 - val_loss: 11.7421 - val_acc: 0.1300\n",
      "Epoch 142/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.2322 - acc: 0.3750 - val_loss: 11.7312 - val_acc: 0.1200\n",
      "Epoch 143/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2286 - acc: 0.3870 - val_loss: 11.7387 - val_acc: 0.1300\n",
      "Epoch 144/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.2272 - acc: 0.3890 - val_loss: 11.7494 - val_acc: 0.1200\n",
      "Epoch 145/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2257 - acc: 0.3890 - val_loss: 11.7798 - val_acc: 0.1100\n",
      "Epoch 146/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.2243 - acc: 0.3940 - val_loss: 11.7429 - val_acc: 0.1200\n",
      "Epoch 147/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2252 - acc: 0.3960 - val_loss: 11.7501 - val_acc: 0.1100\n",
      "Epoch 148/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2213 - acc: 0.3900 - val_loss: 11.7625 - val_acc: 0.1300\n",
      "Epoch 149/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2209 - acc: 0.3820 - val_loss: 11.7721 - val_acc: 0.1000\n",
      "Epoch 150/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2166 - acc: 0.3830 - val_loss: 11.7623 - val_acc: 0.1100\n",
      "Epoch 151/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2137 - acc: 0.3870 - val_loss: 11.7681 - val_acc: 0.1300\n",
      "Epoch 152/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2138 - acc: 0.4010 - val_loss: 11.7840 - val_acc: 0.1200\n",
      "Epoch 153/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2144 - acc: 0.3970 - val_loss: 11.7764 - val_acc: 0.1300\n",
      "Epoch 154/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2144 - acc: 0.3890 - val_loss: 11.7721 - val_acc: 0.1000\n",
      "Epoch 155/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.2119 - acc: 0.3940 - val_loss: 11.7817 - val_acc: 0.1400\n",
      "Epoch 156/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2102 - acc: 0.4020 - val_loss: 11.7946 - val_acc: 0.1300\n",
      "Epoch 157/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.2094 - acc: 0.3940 - val_loss: 11.8000 - val_acc: 0.1300\n",
      "Epoch 158/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2116 - acc: 0.4070 - val_loss: 11.7766 - val_acc: 0.1400\n",
      "Epoch 159/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.2065 - acc: 0.3970 - val_loss: 11.7946 - val_acc: 0.1100\n",
      "Epoch 160/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2058 - acc: 0.4140 - val_loss: 11.7930 - val_acc: 0.1300\n",
      "Epoch 161/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.2040 - acc: 0.4090 - val_loss: 11.8039 - val_acc: 0.1200\n",
      "Epoch 162/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2041 - acc: 0.4100 - val_loss: 11.8089 - val_acc: 0.1300\n",
      "Epoch 163/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.2029 - acc: 0.3930 - val_loss: 11.8178 - val_acc: 0.1300\n",
      "Epoch 164/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.2021 - acc: 0.4050 - val_loss: 11.8088 - val_acc: 0.1400\n",
      "Epoch 165/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.2006 - acc: 0.3910 - val_loss: 11.8045 - val_acc: 0.1300\n",
      "Epoch 166/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.2003 - acc: 0.3920 - val_loss: 11.8255 - val_acc: 0.1100\n",
      "Epoch 167/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.1959 - acc: 0.3970 - val_loss: 11.8094 - val_acc: 0.1200\n",
      "Epoch 168/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.1989 - acc: 0.4060 - val_loss: 11.8252 - val_acc: 0.1200\n",
      "Epoch 169/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.2007 - acc: 0.4160 - val_loss: 11.8185 - val_acc: 0.1400\n",
      "Epoch 170/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1997 - acc: 0.3970 - val_loss: 11.8257 - val_acc: 0.1200\n",
      "Epoch 171/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.1976 - acc: 0.3990 - val_loss: 11.8251 - val_acc: 0.1500\n",
      "Epoch 172/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.1949 - acc: 0.4040 - val_loss: 11.8533 - val_acc: 0.1200\n",
      "Epoch 173/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.1920 - acc: 0.3990 - val_loss: 11.8269 - val_acc: 0.1200\n",
      "Epoch 174/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.1915 - acc: 0.4280 - val_loss: 11.8297 - val_acc: 0.1300\n",
      "Epoch 175/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.1895 - acc: 0.4040 - val_loss: 11.8357 - val_acc: 0.1200\n",
      "Epoch 176/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.1870 - acc: 0.4200 - val_loss: 11.8314 - val_acc: 0.1500\n",
      "Epoch 177/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 11.1840 - acc: 0.4200 - val_loss: 11.8403 - val_acc: 0.1200\n",
      "Epoch 178/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.1854 - acc: 0.4130 - val_loss: 11.8533 - val_acc: 0.1300\n",
      "Epoch 179/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1832 - acc: 0.4070 - val_loss: 11.8514 - val_acc: 0.1200\n",
      "Epoch 180/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.1808 - acc: 0.4200 - val_loss: 11.8605 - val_acc: 0.1600\n",
      "Epoch 181/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.1827 - acc: 0.4220 - val_loss: 11.8651 - val_acc: 0.1400\n",
      "Epoch 182/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1819 - acc: 0.4290 - val_loss: 11.8684 - val_acc: 0.1400\n",
      "Epoch 183/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.1798 - acc: 0.4260 - val_loss: 11.8540 - val_acc: 0.1500\n",
      "Epoch 184/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1789 - acc: 0.4280 - val_loss: 11.8786 - val_acc: 0.1600\n",
      "Epoch 185/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1784 - acc: 0.4240 - val_loss: 11.8672 - val_acc: 0.1400\n",
      "Epoch 186/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.1774 - acc: 0.4180 - val_loss: 11.8753 - val_acc: 0.1500\n",
      "Epoch 187/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.1763 - acc: 0.4050 - val_loss: 11.8955 - val_acc: 0.1300\n",
      "Epoch 188/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.1710 - acc: 0.4120 - val_loss: 11.8793 - val_acc: 0.1200\n",
      "Epoch 189/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1729 - acc: 0.4360 - val_loss: 11.9072 - val_acc: 0.1300\n",
      "Epoch 190/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1723 - acc: 0.4140 - val_loss: 11.8765 - val_acc: 0.1400\n",
      "Epoch 191/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1710 - acc: 0.4280 - val_loss: 11.8909 - val_acc: 0.1400\n",
      "Epoch 192/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.1698 - acc: 0.4240 - val_loss: 11.9054 - val_acc: 0.1600\n",
      "Epoch 193/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.1685 - acc: 0.4220 - val_loss: 11.9068 - val_acc: 0.1300\n",
      "Epoch 194/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.1715 - acc: 0.4280 - val_loss: 11.8834 - val_acc: 0.1400\n",
      "Epoch 195/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 11.1699 - acc: 0.4160 - val_loss: 11.8992 - val_acc: 0.1300\n",
      "Epoch 196/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.1656 - acc: 0.4370 - val_loss: 11.9023 - val_acc: 0.1300\n",
      "Epoch 197/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.1635 - acc: 0.4200 - val_loss: 11.8959 - val_acc: 0.1400\n",
      "Epoch 198/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.1655 - acc: 0.4230 - val_loss: 11.9236 - val_acc: 0.1200\n",
      "Epoch 199/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.1631 - acc: 0.4340 - val_loss: 11.9041 - val_acc: 0.1300\n",
      "Epoch 200/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.1605 - acc: 0.4280 - val_loss: 11.9124 - val_acc: 0.1500\n",
      "Epoch 201/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 11.1611 - acc: 0.4280 - val_loss: 11.9164 - val_acc: 0.1500\n",
      "Epoch 202/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.1613 - acc: 0.4190 - val_loss: 11.9122 - val_acc: 0.1300\n",
      "Epoch 203/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 11.1582 - acc: 0.4240 - val_loss: 11.9239 - val_acc: 0.1200\n",
      "Epoch 204/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 11.1587 - acc: 0.4340 - val_loss: 11.9323 - val_acc: 0.1300\n",
      "Epoch 205/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.1587 - acc: 0.4360 - val_loss: 11.9213 - val_acc: 0.1200\n",
      "Epoch 206/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 11.1575 - acc: 0.4170 - val_loss: 11.9445 - val_acc: 0.1300\n",
      "Epoch 207/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 11.1560 - acc: 0.4220 - val_loss: 11.9453 - val_acc: 0.1500\n",
      "Epoch 208/1000\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 11.1562 - acc: 0.4360 - val_loss: 11.9416 - val_acc: 0.1400\n",
      "Epoch 209/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.1531 - acc: 0.4150 - val_loss: 11.9322 - val_acc: 0.1400\n",
      "Epoch 210/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 11.1514 - acc: 0.4240 - val_loss: 11.9431 - val_acc: 0.1600\n",
      "Epoch 211/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 11.1522 - acc: 0.4320 - val_loss: 11.9546 - val_acc: 0.1400\n",
      "Epoch 212/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 11.1499 - acc: 0.4200 - val_loss: 11.9436 - val_acc: 0.1300\n",
      "Epoch 213/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.1492 - acc: 0.4260 - val_loss: 11.9539 - val_acc: 0.1300\n",
      "Epoch 214/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 11.1500 - acc: 0.4380 - val_loss: 11.9546 - val_acc: 0.1300\n",
      "Epoch 215/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 11.5625 - acc: 0.40 - 0s 43us/step - loss: 11.1503 - acc: 0.4360 - val_loss: 11.9465 - val_acc: 0.1800\n",
      "Epoch 216/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.1476 - acc: 0.4470 - val_loss: 11.9425 - val_acc: 0.1400\n",
      "Epoch 217/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.1484 - acc: 0.4360 - val_loss: 11.9745 - val_acc: 0.1100\n",
      "Epoch 218/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 11.1437 - acc: 0.4430 - val_loss: 11.9733 - val_acc: 0.1200\n",
      "Epoch 219/1000\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 11.1458 - acc: 0.4500 - val_loss: 11.9747 - val_acc: 0.1600\n",
      "Epoch 220/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 11.1460 - acc: 0.4420 - val_loss: 11.9831 - val_acc: 0.1400\n",
      "Epoch 221/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 11.1450 - acc: 0.4430 - val_loss: 11.9770 - val_acc: 0.1400\n",
      "Epoch 222/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 11.1429 - acc: 0.4380 - val_loss: 11.9758 - val_acc: 0.1200\n",
      "Epoch 223/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 11.1426 - acc: 0.4390 - val_loss: 11.9743 - val_acc: 0.1500\n",
      "Epoch 224/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.1406 - acc: 0.4480 - val_loss: 11.9751 - val_acc: 0.1500\n",
      "Epoch 225/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.1410 - acc: 0.4260 - val_loss: 11.9985 - val_acc: 0.1500\n",
      "Epoch 226/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.1389 - acc: 0.4470 - val_loss: 11.9790 - val_acc: 0.1600\n",
      "Epoch 227/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 11.1375 - acc: 0.4420 - val_loss: 11.9909 - val_acc: 0.1300\n",
      "Epoch 228/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 11.1371 - acc: 0.4350 - val_loss: 11.9938 - val_acc: 0.1400\n",
      "Epoch 229/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.1376 - acc: 0.4460 - val_loss: 11.9932 - val_acc: 0.1600\n",
      "Epoch 230/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.1374 - acc: 0.4400 - val_loss: 11.9984 - val_acc: 0.1400\n",
      "Epoch 231/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.1372 - acc: 0.4270 - val_loss: 11.9960 - val_acc: 0.1300\n",
      "Epoch 232/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 11.1370 - acc: 0.4480 - val_loss: 11.9972 - val_acc: 0.1400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 11.1357 - acc: 0.4310 - val_loss: 12.0227 - val_acc: 0.1400\n",
      "Epoch 234/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.1344 - acc: 0.4420 - val_loss: 12.0088 - val_acc: 0.1400\n",
      "Epoch 235/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1326 - acc: 0.4440 - val_loss: 12.0135 - val_acc: 0.1600\n",
      "Epoch 236/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.1320 - acc: 0.4460 - val_loss: 12.0114 - val_acc: 0.1500\n",
      "Epoch 237/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1303 - acc: 0.4580 - val_loss: 12.0045 - val_acc: 0.1500\n",
      "Epoch 238/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1307 - acc: 0.4510 - val_loss: 12.0200 - val_acc: 0.1400\n",
      "Epoch 239/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1288 - acc: 0.4290 - val_loss: 12.0427 - val_acc: 0.1300\n",
      "Epoch 240/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.1310 - acc: 0.4450 - val_loss: 12.0208 - val_acc: 0.1400\n",
      "Epoch 241/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1288 - acc: 0.4620 - val_loss: 12.0281 - val_acc: 0.1300\n",
      "Epoch 242/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.1308 - acc: 0.4410 - val_loss: 12.0301 - val_acc: 0.1200\n",
      "Epoch 243/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1299 - acc: 0.4470 - val_loss: 12.0246 - val_acc: 0.1400\n",
      "Epoch 244/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.1281 - acc: 0.4470 - val_loss: 12.0240 - val_acc: 0.1400\n",
      "Epoch 245/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1257 - acc: 0.4560 - val_loss: 12.0400 - val_acc: 0.1200\n",
      "Epoch 246/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1244 - acc: 0.4540 - val_loss: 12.0237 - val_acc: 0.1400\n",
      "Epoch 247/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 11.1270 - acc: 0.4750 - val_loss: 12.0682 - val_acc: 0.1300\n",
      "Epoch 248/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.1244 - acc: 0.4510 - val_loss: 12.0561 - val_acc: 0.1500\n",
      "Epoch 249/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.1239 - acc: 0.4510 - val_loss: 12.0407 - val_acc: 0.1500\n",
      "Epoch 250/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1205 - acc: 0.4600 - val_loss: 12.0388 - val_acc: 0.1400\n",
      "Epoch 251/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1207 - acc: 0.4580 - val_loss: 12.0377 - val_acc: 0.1500\n",
      "Epoch 252/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.1200 - acc: 0.4600 - val_loss: 12.0421 - val_acc: 0.1600\n",
      "Epoch 253/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.1212 - acc: 0.4500 - val_loss: 12.0539 - val_acc: 0.1300\n",
      "Epoch 254/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.1202 - acc: 0.4640 - val_loss: 12.0404 - val_acc: 0.1500\n",
      "Epoch 255/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.1235 - acc: 0.4440 - val_loss: 12.0408 - val_acc: 0.1600\n",
      "Epoch 256/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1207 - acc: 0.4490 - val_loss: 12.0656 - val_acc: 0.1400\n",
      "Epoch 257/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1196 - acc: 0.4480 - val_loss: 12.0459 - val_acc: 0.1400\n",
      "Epoch 258/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.1182 - acc: 0.4650 - val_loss: 12.0552 - val_acc: 0.1300\n",
      "Epoch 259/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.1169 - acc: 0.4570 - val_loss: 12.0738 - val_acc: 0.1500\n",
      "Epoch 260/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1154 - acc: 0.4500 - val_loss: 12.0734 - val_acc: 0.1500\n",
      "Epoch 261/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.1146 - acc: 0.4640 - val_loss: 12.0571 - val_acc: 0.1400\n",
      "Epoch 262/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1191 - acc: 0.4480 - val_loss: 12.0561 - val_acc: 0.1700\n",
      "Epoch 263/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1181 - acc: 0.4630 - val_loss: 12.0619 - val_acc: 0.1500\n",
      "Epoch 264/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.1161 - acc: 0.4620 - val_loss: 12.0676 - val_acc: 0.1400\n",
      "Epoch 265/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1144 - acc: 0.4430 - val_loss: 12.0715 - val_acc: 0.1700\n",
      "Epoch 266/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1179 - acc: 0.4620 - val_loss: 12.0732 - val_acc: 0.1600\n",
      "Epoch 267/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1134 - acc: 0.4590 - val_loss: 12.0962 - val_acc: 0.1400\n",
      "Epoch 268/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1112 - acc: 0.4450 - val_loss: 12.0836 - val_acc: 0.1600\n",
      "Epoch 269/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1122 - acc: 0.4510 - val_loss: 12.0877 - val_acc: 0.1700\n",
      "Epoch 270/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.1117 - acc: 0.4480 - val_loss: 12.0755 - val_acc: 0.1400\n",
      "Epoch 271/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1100 - acc: 0.4550 - val_loss: 12.0834 - val_acc: 0.1600\n",
      "Epoch 272/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1093 - acc: 0.4490 - val_loss: 12.0934 - val_acc: 0.1400\n",
      "Epoch 273/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.1108 - acc: 0.4550 - val_loss: 12.0722 - val_acc: 0.1400\n",
      "Epoch 274/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.1096 - acc: 0.4660 - val_loss: 12.0797 - val_acc: 0.1400\n",
      "Epoch 275/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1086 - acc: 0.4560 - val_loss: 12.0679 - val_acc: 0.1700\n",
      "Epoch 276/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.1063 - acc: 0.4730 - val_loss: 12.0974 - val_acc: 0.1600\n",
      "Epoch 277/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.1077 - acc: 0.4660 - val_loss: 12.0980 - val_acc: 0.1500\n",
      "Epoch 278/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.1070 - acc: 0.4730 - val_loss: 12.0855 - val_acc: 0.1600\n",
      "Epoch 279/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1086 - acc: 0.4620 - val_loss: 12.0902 - val_acc: 0.1500\n",
      "Epoch 280/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.1065 - acc: 0.4520 - val_loss: 12.1072 - val_acc: 0.1600\n",
      "Epoch 281/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.1066 - acc: 0.4720 - val_loss: 12.0868 - val_acc: 0.1600\n",
      "Epoch 282/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.1022 - acc: 0.4470 - val_loss: 12.1089 - val_acc: 0.1500\n",
      "Epoch 283/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.1028 - acc: 0.4730 - val_loss: 12.0831 - val_acc: 0.1600\n",
      "Epoch 284/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.1040 - acc: 0.4700 - val_loss: 12.1129 - val_acc: 0.1500\n",
      "Epoch 285/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.1002 - acc: 0.4690 - val_loss: 12.0997 - val_acc: 0.1800\n",
      "Epoch 286/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.1040 - acc: 0.4640 - val_loss: 12.1045 - val_acc: 0.1600\n",
      "Epoch 287/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.1016 - acc: 0.4700 - val_loss: 12.1140 - val_acc: 0.1700\n",
      "Epoch 288/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.1043 - acc: 0.4590 - val_loss: 12.1217 - val_acc: 0.1600\n",
      "Epoch 289/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.1022 - acc: 0.4510 - val_loss: 12.1075 - val_acc: 0.1600\n",
      "Epoch 290/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.1003 - acc: 0.4550 - val_loss: 12.1081 - val_acc: 0.1600\n",
      "Epoch 291/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0984 - acc: 0.4700 - val_loss: 12.1371 - val_acc: 0.1600\n",
      "Epoch 292/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0997 - acc: 0.4740 - val_loss: 12.1302 - val_acc: 0.1300\n",
      "Epoch 293/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0979 - acc: 0.4640 - val_loss: 12.1441 - val_acc: 0.1800\n",
      "Epoch 294/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0981 - acc: 0.4660 - val_loss: 12.1276 - val_acc: 0.1600\n",
      "Epoch 295/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0980 - acc: 0.4640 - val_loss: 12.1166 - val_acc: 0.1700\n",
      "Epoch 296/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0944 - acc: 0.4630 - val_loss: 12.1370 - val_acc: 0.1700\n",
      "Epoch 297/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0978 - acc: 0.4690 - val_loss: 12.1153 - val_acc: 0.1700\n",
      "Epoch 298/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0970 - acc: 0.4620 - val_loss: 12.1270 - val_acc: 0.1600\n",
      "Epoch 299/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0983 - acc: 0.4690 - val_loss: 12.1246 - val_acc: 0.1600\n",
      "Epoch 300/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0969 - acc: 0.4540 - val_loss: 12.1245 - val_acc: 0.1600\n",
      "Epoch 301/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0944 - acc: 0.4730 - val_loss: 12.1426 - val_acc: 0.1500\n",
      "Epoch 302/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0924 - acc: 0.4670 - val_loss: 12.1303 - val_acc: 0.1600\n",
      "Epoch 303/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0934 - acc: 0.4550 - val_loss: 12.1529 - val_acc: 0.1600\n",
      "Epoch 304/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0920 - acc: 0.4590 - val_loss: 12.1585 - val_acc: 0.1600\n",
      "Epoch 305/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0938 - acc: 0.4610 - val_loss: 12.1448 - val_acc: 0.1600\n",
      "Epoch 306/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0936 - acc: 0.4810 - val_loss: 12.1516 - val_acc: 0.1600\n",
      "Epoch 307/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0911 - acc: 0.4770 - val_loss: 12.1549 - val_acc: 0.1600\n",
      "Epoch 308/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0936 - acc: 0.4750 - val_loss: 12.1695 - val_acc: 0.1800\n",
      "Epoch 309/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0893 - acc: 0.4870 - val_loss: 12.1538 - val_acc: 0.1500\n",
      "Epoch 310/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0918 - acc: 0.4770 - val_loss: 12.1501 - val_acc: 0.1500\n",
      "Epoch 311/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0903 - acc: 0.4840 - val_loss: 12.1536 - val_acc: 0.1600\n",
      "Epoch 312/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0890 - acc: 0.4760 - val_loss: 12.1706 - val_acc: 0.1500\n",
      "Epoch 313/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0869 - acc: 0.4830 - val_loss: 12.1649 - val_acc: 0.1600\n",
      "Epoch 314/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0905 - acc: 0.4750 - val_loss: 12.1709 - val_acc: 0.1600\n",
      "Epoch 315/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0897 - acc: 0.4640 - val_loss: 12.1762 - val_acc: 0.1500\n",
      "Epoch 316/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0861 - acc: 0.4820 - val_loss: 12.1587 - val_acc: 0.1600\n",
      "Epoch 317/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0885 - acc: 0.4650 - val_loss: 12.1547 - val_acc: 0.1600\n",
      "Epoch 318/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0871 - acc: 0.4700 - val_loss: 12.1464 - val_acc: 0.1600\n",
      "Epoch 319/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0881 - acc: 0.4610 - val_loss: 12.1846 - val_acc: 0.1500\n",
      "Epoch 320/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0842 - acc: 0.4810 - val_loss: 12.1836 - val_acc: 0.1800\n",
      "Epoch 321/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0842 - acc: 0.4710 - val_loss: 12.1770 - val_acc: 0.1500\n",
      "Epoch 322/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0866 - acc: 0.4760 - val_loss: 12.1879 - val_acc: 0.1500\n",
      "Epoch 323/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0868 - acc: 0.4860 - val_loss: 12.1714 - val_acc: 0.1400\n",
      "Epoch 324/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0883 - acc: 0.4920 - val_loss: 12.1896 - val_acc: 0.1500\n",
      "Epoch 325/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0856 - acc: 0.4890 - val_loss: 12.1878 - val_acc: 0.1600\n",
      "Epoch 326/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0824 - acc: 0.4790 - val_loss: 12.1901 - val_acc: 0.1800\n",
      "Epoch 327/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0815 - acc: 0.4850 - val_loss: 12.1844 - val_acc: 0.1700\n",
      "Epoch 328/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0838 - acc: 0.4870 - val_loss: 12.2015 - val_acc: 0.1500\n",
      "Epoch 329/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0836 - acc: 0.4590 - val_loss: 12.1992 - val_acc: 0.1500\n",
      "Epoch 330/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0818 - acc: 0.4830 - val_loss: 12.1934 - val_acc: 0.1600\n",
      "Epoch 331/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0821 - acc: 0.4780 - val_loss: 12.1899 - val_acc: 0.1700\n",
      "Epoch 332/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0814 - acc: 0.4770 - val_loss: 12.1985 - val_acc: 0.1600\n",
      "Epoch 333/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0853 - acc: 0.4660 - val_loss: 12.1986 - val_acc: 0.1800\n",
      "Epoch 334/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0831 - acc: 0.4870 - val_loss: 12.1737 - val_acc: 0.1700\n",
      "Epoch 335/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0799 - acc: 0.4920 - val_loss: 12.1975 - val_acc: 0.1500\n",
      "Epoch 336/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0778 - acc: 0.4860 - val_loss: 12.2177 - val_acc: 0.1400\n",
      "Epoch 337/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.0796 - acc: 0.4800 - val_loss: 12.1981 - val_acc: 0.1600\n",
      "Epoch 338/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0797 - acc: 0.4770 - val_loss: 12.2351 - val_acc: 0.1500\n",
      "Epoch 339/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0764 - acc: 0.4970 - val_loss: 12.2257 - val_acc: 0.1700\n",
      "Epoch 340/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0769 - acc: 0.4790 - val_loss: 12.2211 - val_acc: 0.1400\n",
      "Epoch 341/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.0766 - acc: 0.4770 - val_loss: 12.2068 - val_acc: 0.1500\n",
      "Epoch 342/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0783 - acc: 0.4980 - val_loss: 12.2068 - val_acc: 0.1400\n",
      "Epoch 343/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0776 - acc: 0.4810 - val_loss: 12.2050 - val_acc: 0.1600\n",
      "Epoch 344/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0754 - acc: 0.4990 - val_loss: 12.2278 - val_acc: 0.1300\n",
      "Epoch 345/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0769 - acc: 0.4900 - val_loss: 12.2267 - val_acc: 0.1500\n",
      "Epoch 346/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0739 - acc: 0.4870 - val_loss: 12.2212 - val_acc: 0.1400\n",
      "Epoch 347/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0751 - acc: 0.4910 - val_loss: 12.2381 - val_acc: 0.1400\n",
      "Epoch 348/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0762 - acc: 0.4920 - val_loss: 12.2166 - val_acc: 0.1500\n",
      "Epoch 349/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 45us/step - loss: 11.0727 - acc: 0.4880 - val_loss: 12.2230 - val_acc: 0.1600\n",
      "Epoch 350/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0718 - acc: 0.4930 - val_loss: 12.2360 - val_acc: 0.1500\n",
      "Epoch 351/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0731 - acc: 0.4830 - val_loss: 12.2184 - val_acc: 0.1800\n",
      "Epoch 352/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0733 - acc: 0.4830 - val_loss: 12.2384 - val_acc: 0.1600\n",
      "Epoch 353/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0736 - acc: 0.4910 - val_loss: 12.2429 - val_acc: 0.1500\n",
      "Epoch 354/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0702 - acc: 0.4890 - val_loss: 12.2246 - val_acc: 0.1500\n",
      "Epoch 355/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0723 - acc: 0.4890 - val_loss: 12.2407 - val_acc: 0.1400\n",
      "Epoch 356/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0730 - acc: 0.4940 - val_loss: 12.2474 - val_acc: 0.1600\n",
      "Epoch 357/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0727 - acc: 0.4870 - val_loss: 12.2653 - val_acc: 0.1500\n",
      "Epoch 358/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0708 - acc: 0.4940 - val_loss: 12.2579 - val_acc: 0.1400\n",
      "Epoch 359/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0730 - acc: 0.4820 - val_loss: 12.2567 - val_acc: 0.1500\n",
      "Epoch 360/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0723 - acc: 0.4960 - val_loss: 12.2635 - val_acc: 0.1600\n",
      "Epoch 361/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0685 - acc: 0.5020 - val_loss: 12.2753 - val_acc: 0.1600\n",
      "Epoch 362/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0700 - acc: 0.4970 - val_loss: 12.2915 - val_acc: 0.1600\n",
      "Epoch 363/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0697 - acc: 0.4880 - val_loss: 12.2660 - val_acc: 0.1500\n",
      "Epoch 364/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0691 - acc: 0.4790 - val_loss: 12.2621 - val_acc: 0.1700\n",
      "Epoch 365/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0695 - acc: 0.4940 - val_loss: 12.2720 - val_acc: 0.1200\n",
      "Epoch 366/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0711 - acc: 0.4850 - val_loss: 12.2727 - val_acc: 0.1600\n",
      "Epoch 367/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0680 - acc: 0.4950 - val_loss: 12.2597 - val_acc: 0.1500\n",
      "Epoch 368/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0673 - acc: 0.5050 - val_loss: 12.2814 - val_acc: 0.1700\n",
      "Epoch 369/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0670 - acc: 0.4810 - val_loss: 12.2701 - val_acc: 0.1600\n",
      "Epoch 370/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0666 - acc: 0.4960 - val_loss: 12.2505 - val_acc: 0.1700\n",
      "Epoch 371/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0639 - acc: 0.4870 - val_loss: 12.2705 - val_acc: 0.1600\n",
      "Epoch 372/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0666 - acc: 0.4950 - val_loss: 12.2775 - val_acc: 0.1700\n",
      "Epoch 373/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0665 - acc: 0.5030 - val_loss: 12.2847 - val_acc: 0.1600\n",
      "Epoch 374/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0664 - acc: 0.5010 - val_loss: 12.2913 - val_acc: 0.1600\n",
      "Epoch 375/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0634 - acc: 0.4800 - val_loss: 12.2790 - val_acc: 0.1800\n",
      "Epoch 376/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0638 - acc: 0.4850 - val_loss: 12.2970 - val_acc: 0.1600\n",
      "Epoch 377/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0642 - acc: 0.4780 - val_loss: 12.3004 - val_acc: 0.1600\n",
      "Epoch 378/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0631 - acc: 0.4770 - val_loss: 12.2893 - val_acc: 0.1500\n",
      "Epoch 379/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0681 - acc: 0.5150 - val_loss: 12.2755 - val_acc: 0.1500\n",
      "Epoch 380/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0632 - acc: 0.4710 - val_loss: 12.2698 - val_acc: 0.1300\n",
      "Epoch 381/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0623 - acc: 0.4870 - val_loss: 12.3015 - val_acc: 0.1400\n",
      "Epoch 382/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0620 - acc: 0.4970 - val_loss: 12.2905 - val_acc: 0.1300\n",
      "Epoch 383/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0631 - acc: 0.4940 - val_loss: 12.3040 - val_acc: 0.1300\n",
      "Epoch 384/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0609 - acc: 0.5160 - val_loss: 12.2892 - val_acc: 0.1400\n",
      "Epoch 385/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0616 - acc: 0.5030 - val_loss: 12.3086 - val_acc: 0.1800\n",
      "Epoch 386/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0589 - acc: 0.5040 - val_loss: 12.3100 - val_acc: 0.1400\n",
      "Epoch 387/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0631 - acc: 0.4740 - val_loss: 12.3185 - val_acc: 0.1600\n",
      "Epoch 388/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0603 - acc: 0.4880 - val_loss: 12.3035 - val_acc: 0.1600\n",
      "Epoch 389/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0606 - acc: 0.4780 - val_loss: 12.3239 - val_acc: 0.1500\n",
      "Epoch 390/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0591 - acc: 0.5000 - val_loss: 12.3041 - val_acc: 0.1500\n",
      "Epoch 391/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0611 - acc: 0.4820 - val_loss: 12.3143 - val_acc: 0.1400\n",
      "Epoch 392/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0588 - acc: 0.4770 - val_loss: 12.3267 - val_acc: 0.1800\n",
      "Epoch 393/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0605 - acc: 0.5020 - val_loss: 12.3160 - val_acc: 0.1400\n",
      "Epoch 394/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0650 - acc: 0.5010 - val_loss: 12.3095 - val_acc: 0.1600\n",
      "Epoch 395/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0617 - acc: 0.4730 - val_loss: 12.3138 - val_acc: 0.1500\n",
      "Epoch 396/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0590 - acc: 0.5010 - val_loss: 12.3140 - val_acc: 0.1300\n",
      "Epoch 397/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0574 - acc: 0.4950 - val_loss: 12.3100 - val_acc: 0.1500\n",
      "Epoch 398/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0583 - acc: 0.4850 - val_loss: 12.3072 - val_acc: 0.1600\n",
      "Epoch 399/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0592 - acc: 0.4980 - val_loss: 12.3240 - val_acc: 0.1600\n",
      "Epoch 400/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0555 - acc: 0.5150 - val_loss: 12.3215 - val_acc: 0.1500\n",
      "Epoch 401/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0551 - acc: 0.4860 - val_loss: 12.3387 - val_acc: 0.1600\n",
      "Epoch 402/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0575 - acc: 0.5070 - val_loss: 12.3280 - val_acc: 0.1400\n",
      "Epoch 403/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0587 - acc: 0.4920 - val_loss: 12.3631 - val_acc: 0.1700\n",
      "Epoch 404/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0560 - acc: 0.4980 - val_loss: 12.3222 - val_acc: 0.1400\n",
      "Epoch 405/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0559 - acc: 0.4910 - val_loss: 12.3471 - val_acc: 0.1300\n",
      "Epoch 406/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0542 - acc: 0.5090 - val_loss: 12.3126 - val_acc: 0.1500\n",
      "Epoch 407/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0541 - acc: 0.5050 - val_loss: 12.3293 - val_acc: 0.1700\n",
      "Epoch 408/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0554 - acc: 0.5170 - val_loss: 12.3534 - val_acc: 0.1300\n",
      "Epoch 409/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0541 - acc: 0.4990 - val_loss: 12.3374 - val_acc: 0.1500\n",
      "Epoch 410/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0560 - acc: 0.4990 - val_loss: 12.3525 - val_acc: 0.1400\n",
      "Epoch 411/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0523 - acc: 0.5050 - val_loss: 12.3538 - val_acc: 0.1800\n",
      "Epoch 412/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0520 - acc: 0.5000 - val_loss: 12.3475 - val_acc: 0.1500\n",
      "Epoch 413/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0522 - acc: 0.5010 - val_loss: 12.3499 - val_acc: 0.1500\n",
      "Epoch 414/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0520 - acc: 0.5170 - val_loss: 12.3539 - val_acc: 0.1500\n",
      "Epoch 415/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0535 - acc: 0.4880 - val_loss: 12.3788 - val_acc: 0.1600\n",
      "Epoch 416/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0517 - acc: 0.4900 - val_loss: 12.3628 - val_acc: 0.1300\n",
      "Epoch 417/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0543 - acc: 0.5050 - val_loss: 12.3539 - val_acc: 0.1600\n",
      "Epoch 418/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0541 - acc: 0.5000 - val_loss: 12.3578 - val_acc: 0.1500\n",
      "Epoch 419/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0541 - acc: 0.4910 - val_loss: 12.3598 - val_acc: 0.1600\n",
      "Epoch 420/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0525 - acc: 0.5010 - val_loss: 12.3853 - val_acc: 0.1200\n",
      "Epoch 421/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0516 - acc: 0.4980 - val_loss: 12.3724 - val_acc: 0.1700\n",
      "Epoch 422/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0526 - acc: 0.5050 - val_loss: 12.3679 - val_acc: 0.1400\n",
      "Epoch 423/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0492 - acc: 0.5020 - val_loss: 12.3800 - val_acc: 0.1300\n",
      "Epoch 424/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0528 - acc: 0.4950 - val_loss: 12.3634 - val_acc: 0.1200\n",
      "Epoch 425/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0505 - acc: 0.4950 - val_loss: 12.3830 - val_acc: 0.1300\n",
      "Epoch 426/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0489 - acc: 0.5180 - val_loss: 12.3881 - val_acc: 0.1400\n",
      "Epoch 427/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0521 - acc: 0.4990 - val_loss: 12.3948 - val_acc: 0.1600\n",
      "Epoch 428/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0493 - acc: 0.4870 - val_loss: 12.3856 - val_acc: 0.1500\n",
      "Epoch 429/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0476 - acc: 0.4820 - val_loss: 12.3898 - val_acc: 0.1400\n",
      "Epoch 430/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 10.9783 - acc: 0.56 - 0s 37us/step - loss: 11.0516 - acc: 0.5080 - val_loss: 12.3948 - val_acc: 0.1600\n",
      "Epoch 431/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0476 - acc: 0.5070 - val_loss: 12.3973 - val_acc: 0.1700\n",
      "Epoch 432/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0473 - acc: 0.4840 - val_loss: 12.3872 - val_acc: 0.1100\n",
      "Epoch 433/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0462 - acc: 0.5140 - val_loss: 12.4125 - val_acc: 0.1300\n",
      "Epoch 434/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0476 - acc: 0.5120 - val_loss: 12.3904 - val_acc: 0.1300\n",
      "Epoch 435/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0499 - acc: 0.4990 - val_loss: 12.4130 - val_acc: 0.1400\n",
      "Epoch 436/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0462 - acc: 0.5060 - val_loss: 12.4001 - val_acc: 0.1100\n",
      "Epoch 437/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0460 - acc: 0.5080 - val_loss: 12.4076 - val_acc: 0.1700\n",
      "Epoch 438/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0462 - acc: 0.5070 - val_loss: 12.3903 - val_acc: 0.1500\n",
      "Epoch 439/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0449 - acc: 0.5010 - val_loss: 12.4110 - val_acc: 0.1500\n",
      "Epoch 440/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0450 - acc: 0.5060 - val_loss: 12.4152 - val_acc: 0.1200\n",
      "Epoch 441/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0436 - acc: 0.5060 - val_loss: 12.3891 - val_acc: 0.1300\n",
      "Epoch 442/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0428 - acc: 0.5050 - val_loss: 12.3961 - val_acc: 0.1500\n",
      "Epoch 443/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0415 - acc: 0.5210 - val_loss: 12.4121 - val_acc: 0.1200\n",
      "Epoch 444/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0426 - acc: 0.5010 - val_loss: 12.4027 - val_acc: 0.1400\n",
      "Epoch 445/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0444 - acc: 0.4990 - val_loss: 12.4183 - val_acc: 0.1300\n",
      "Epoch 446/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0457 - acc: 0.5230 - val_loss: 12.4175 - val_acc: 0.1100\n",
      "Epoch 447/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0432 - acc: 0.5180 - val_loss: 12.4346 - val_acc: 0.1300\n",
      "Epoch 448/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0436 - acc: 0.5150 - val_loss: 12.4422 - val_acc: 0.1100\n",
      "Epoch 449/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0414 - acc: 0.5170 - val_loss: 12.4286 - val_acc: 0.1500\n",
      "Epoch 450/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0432 - acc: 0.4860 - val_loss: 12.4315 - val_acc: 0.1400\n",
      "Epoch 451/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0421 - acc: 0.5100 - val_loss: 12.4217 - val_acc: 0.1300\n",
      "Epoch 452/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0410 - acc: 0.5170 - val_loss: 12.4271 - val_acc: 0.1200\n",
      "Epoch 453/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0445 - acc: 0.5150 - val_loss: 12.4247 - val_acc: 0.1200\n",
      "Epoch 454/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0406 - acc: 0.5100 - val_loss: 12.4504 - val_acc: 0.1300\n",
      "Epoch 455/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0388 - acc: 0.5050 - val_loss: 12.4247 - val_acc: 0.1500\n",
      "Epoch 456/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0400 - acc: 0.5070 - val_loss: 12.4605 - val_acc: 0.1600\n",
      "Epoch 457/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0426 - acc: 0.5260 - val_loss: 12.4591 - val_acc: 0.1300\n",
      "Epoch 458/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0421 - acc: 0.5110 - val_loss: 12.4284 - val_acc: 0.1400\n",
      "Epoch 459/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0401 - acc: 0.5200 - val_loss: 12.4457 - val_acc: 0.1200\n",
      "Epoch 460/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0395 - acc: 0.5160 - val_loss: 12.4434 - val_acc: 0.1300\n",
      "Epoch 461/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0378 - acc: 0.5210 - val_loss: 12.4278 - val_acc: 0.1700\n",
      "Epoch 462/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0382 - acc: 0.5170 - val_loss: 12.4469 - val_acc: 0.1200\n",
      "Epoch 463/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0366 - acc: 0.5250 - val_loss: 12.4663 - val_acc: 0.1300\n",
      "Epoch 464/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0393 - acc: 0.5070 - val_loss: 12.4712 - val_acc: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0393 - acc: 0.5060 - val_loss: 12.4394 - val_acc: 0.1400\n",
      "Epoch 466/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0383 - acc: 0.5090 - val_loss: 12.4622 - val_acc: 0.1100\n",
      "Epoch 467/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0402 - acc: 0.5140 - val_loss: 12.4698 - val_acc: 0.1300\n",
      "Epoch 468/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0404 - acc: 0.5150 - val_loss: 12.4511 - val_acc: 0.1000\n",
      "Epoch 469/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0359 - acc: 0.4900 - val_loss: 12.4590 - val_acc: 0.0900\n",
      "Epoch 470/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0375 - acc: 0.5270 - val_loss: 12.4609 - val_acc: 0.1200\n",
      "Epoch 471/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0355 - acc: 0.5180 - val_loss: 12.4640 - val_acc: 0.1100\n",
      "Epoch 472/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0375 - acc: 0.5160 - val_loss: 12.4769 - val_acc: 0.1000\n",
      "Epoch 473/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0354 - acc: 0.5100 - val_loss: 12.4681 - val_acc: 0.1100\n",
      "Epoch 474/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0357 - acc: 0.4970 - val_loss: 12.4669 - val_acc: 0.1000\n",
      "Epoch 475/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0349 - acc: 0.5130 - val_loss: 12.4864 - val_acc: 0.1000\n",
      "Epoch 476/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0334 - acc: 0.5060 - val_loss: 12.4708 - val_acc: 0.1100\n",
      "Epoch 477/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0357 - acc: 0.5260 - val_loss: 12.4910 - val_acc: 0.1100\n",
      "Epoch 478/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0339 - acc: 0.5190 - val_loss: 12.4650 - val_acc: 0.1100\n",
      "Epoch 479/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0341 - acc: 0.5260 - val_loss: 12.4674 - val_acc: 0.1100\n",
      "Epoch 480/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0349 - acc: 0.5180 - val_loss: 12.4806 - val_acc: 0.1100\n",
      "Epoch 481/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0373 - acc: 0.5060 - val_loss: 12.4652 - val_acc: 0.1200\n",
      "Epoch 482/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0342 - acc: 0.5230 - val_loss: 12.4808 - val_acc: 0.1300\n",
      "Epoch 483/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0342 - acc: 0.5300 - val_loss: 12.4950 - val_acc: 0.1400\n",
      "Epoch 484/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0313 - acc: 0.5150 - val_loss: 12.4877 - val_acc: 0.1300\n",
      "Epoch 485/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0320 - acc: 0.5280 - val_loss: 12.4872 - val_acc: 0.1200\n",
      "Epoch 486/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0324 - acc: 0.5300 - val_loss: 12.4816 - val_acc: 0.0900\n",
      "Epoch 487/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0343 - acc: 0.5270 - val_loss: 12.4834 - val_acc: 0.0900\n",
      "Epoch 488/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0355 - acc: 0.5170 - val_loss: 12.5034 - val_acc: 0.1200\n",
      "Epoch 489/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0322 - acc: 0.5170 - val_loss: 12.5152 - val_acc: 0.1000\n",
      "Epoch 490/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0330 - acc: 0.5080 - val_loss: 12.5010 - val_acc: 0.1000\n",
      "Epoch 491/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0319 - acc: 0.5110 - val_loss: 12.4946 - val_acc: 0.1200\n",
      "Epoch 492/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0327 - acc: 0.5160 - val_loss: 12.4919 - val_acc: 0.1200\n",
      "Epoch 493/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0329 - acc: 0.5150 - val_loss: 12.5057 - val_acc: 0.1200\n",
      "Epoch 494/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0303 - acc: 0.5150 - val_loss: 12.4999 - val_acc: 0.1100\n",
      "Epoch 495/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0322 - acc: 0.5270 - val_loss: 12.5263 - val_acc: 0.1200\n",
      "Epoch 496/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0321 - acc: 0.5420 - val_loss: 12.4988 - val_acc: 0.1000\n",
      "Epoch 497/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0296 - acc: 0.5140 - val_loss: 12.5113 - val_acc: 0.1100\n",
      "Epoch 498/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0303 - acc: 0.5320 - val_loss: 12.5125 - val_acc: 0.1200\n",
      "Epoch 499/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0313 - acc: 0.5340 - val_loss: 12.5436 - val_acc: 0.1200\n",
      "Epoch 500/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0307 - acc: 0.5280 - val_loss: 12.5318 - val_acc: 0.1100\n",
      "Epoch 501/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0272 - acc: 0.5080 - val_loss: 12.5424 - val_acc: 0.1000\n",
      "Epoch 502/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0288 - acc: 0.5230 - val_loss: 12.5312 - val_acc: 0.1100\n",
      "Epoch 503/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0275 - acc: 0.5310 - val_loss: 12.5503 - val_acc: 0.1100\n",
      "Epoch 504/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0258 - acc: 0.5230 - val_loss: 12.5496 - val_acc: 0.1200\n",
      "Epoch 505/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0273 - acc: 0.5300 - val_loss: 12.5245 - val_acc: 0.1200\n",
      "Epoch 506/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0290 - acc: 0.5160 - val_loss: 12.5197 - val_acc: 0.1200\n",
      "Epoch 507/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0288 - acc: 0.5280 - val_loss: 12.5359 - val_acc: 0.1100\n",
      "Epoch 508/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0302 - acc: 0.5210 - val_loss: 12.5331 - val_acc: 0.1100\n",
      "Epoch 509/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0273 - acc: 0.5170 - val_loss: 12.5581 - val_acc: 0.1100\n",
      "Epoch 510/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0271 - acc: 0.5130 - val_loss: 12.5437 - val_acc: 0.1300\n",
      "Epoch 511/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0273 - acc: 0.5280 - val_loss: 12.5583 - val_acc: 0.1200\n",
      "Epoch 512/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0306 - acc: 0.5340 - val_loss: 12.5431 - val_acc: 0.1200\n",
      "Epoch 513/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0263 - acc: 0.4960 - val_loss: 12.5443 - val_acc: 0.1000\n",
      "Epoch 514/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0256 - acc: 0.5320 - val_loss: 12.5720 - val_acc: 0.1100\n",
      "Epoch 515/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0257 - acc: 0.5190 - val_loss: 12.5578 - val_acc: 0.1000\n",
      "Epoch 516/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0262 - acc: 0.5400 - val_loss: 12.5559 - val_acc: 0.1200\n",
      "Epoch 517/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0251 - acc: 0.5070 - val_loss: 12.5774 - val_acc: 0.1000\n",
      "Epoch 518/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0269 - acc: 0.5310 - val_loss: 12.5373 - val_acc: 0.1100\n",
      "Epoch 519/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0247 - acc: 0.5320 - val_loss: 12.5459 - val_acc: 0.1100\n",
      "Epoch 520/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0239 - acc: 0.5040 - val_loss: 12.5710 - val_acc: 0.0900\n",
      "Epoch 521/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.0255 - acc: 0.5320 - val_loss: 12.5705 - val_acc: 0.1200\n",
      "Epoch 522/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0232 - acc: 0.5260 - val_loss: 12.5594 - val_acc: 0.0900\n",
      "Epoch 523/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0241 - acc: 0.5260 - val_loss: 12.5952 - val_acc: 0.0900\n",
      "Epoch 524/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0240 - acc: 0.5090 - val_loss: 12.5781 - val_acc: 0.1200\n",
      "Epoch 525/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0259 - acc: 0.5280 - val_loss: 12.5983 - val_acc: 0.1100\n",
      "Epoch 526/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0230 - acc: 0.5210 - val_loss: 12.5563 - val_acc: 0.1300\n",
      "Epoch 527/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0224 - acc: 0.5340 - val_loss: 12.5697 - val_acc: 0.1300\n",
      "Epoch 528/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0244 - acc: 0.5280 - val_loss: 12.5789 - val_acc: 0.1200\n",
      "Epoch 529/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0233 - acc: 0.5390 - val_loss: 12.5692 - val_acc: 0.0900\n",
      "Epoch 530/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0220 - acc: 0.5360 - val_loss: 12.5832 - val_acc: 0.1000\n",
      "Epoch 531/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0261 - acc: 0.5030 - val_loss: 12.5941 - val_acc: 0.1100\n",
      "Epoch 532/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0209 - acc: 0.5300 - val_loss: 12.6046 - val_acc: 0.1300\n",
      "Epoch 533/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0218 - acc: 0.5280 - val_loss: 12.5729 - val_acc: 0.1500\n",
      "Epoch 534/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0231 - acc: 0.5310 - val_loss: 12.6152 - val_acc: 0.1200\n",
      "Epoch 535/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0213 - acc: 0.5170 - val_loss: 12.5879 - val_acc: 0.1300\n",
      "Epoch 536/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0235 - acc: 0.5520 - val_loss: 12.5994 - val_acc: 0.1000\n",
      "Epoch 537/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0222 - acc: 0.5290 - val_loss: 12.6183 - val_acc: 0.1100\n",
      "Epoch 538/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0223 - acc: 0.5190 - val_loss: 12.5937 - val_acc: 0.0900\n",
      "Epoch 539/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0188 - acc: 0.5340 - val_loss: 12.5963 - val_acc: 0.1200\n",
      "Epoch 540/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0191 - acc: 0.5330 - val_loss: 12.5943 - val_acc: 0.1200\n",
      "Epoch 541/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0208 - acc: 0.5320 - val_loss: 12.6343 - val_acc: 0.1100\n",
      "Epoch 542/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0207 - acc: 0.5300 - val_loss: 12.6102 - val_acc: 0.1300\n",
      "Epoch 543/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0196 - acc: 0.5440 - val_loss: 12.6130 - val_acc: 0.1100\n",
      "Epoch 544/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0203 - acc: 0.5280 - val_loss: 12.6068 - val_acc: 0.1000\n",
      "Epoch 545/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0186 - acc: 0.5500 - val_loss: 12.6312 - val_acc: 0.1100\n",
      "Epoch 546/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0209 - acc: 0.5380 - val_loss: 12.6220 - val_acc: 0.1100\n",
      "Epoch 547/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0185 - acc: 0.5410 - val_loss: 12.6235 - val_acc: 0.1000\n",
      "Epoch 548/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0190 - acc: 0.5440 - val_loss: 12.6140 - val_acc: 0.1300\n",
      "Epoch 549/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0184 - acc: 0.5290 - val_loss: 12.6307 - val_acc: 0.1200\n",
      "Epoch 550/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0174 - acc: 0.5240 - val_loss: 12.6208 - val_acc: 0.1200\n",
      "Epoch 551/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0180 - acc: 0.5300 - val_loss: 12.6192 - val_acc: 0.1100\n",
      "Epoch 552/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0148 - acc: 0.5190 - val_loss: 12.6318 - val_acc: 0.1100\n",
      "Epoch 553/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0167 - acc: 0.5310 - val_loss: 12.6322 - val_acc: 0.1000\n",
      "Epoch 554/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0167 - acc: 0.5280 - val_loss: 12.6293 - val_acc: 0.1200\n",
      "Epoch 555/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0178 - acc: 0.5390 - val_loss: 12.6634 - val_acc: 0.0900\n",
      "Epoch 556/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0175 - acc: 0.5390 - val_loss: 12.6189 - val_acc: 0.1200\n",
      "Epoch 557/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0164 - acc: 0.5300 - val_loss: 12.6265 - val_acc: 0.1200\n",
      "Epoch 558/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0180 - acc: 0.5340 - val_loss: 12.6417 - val_acc: 0.0800\n",
      "Epoch 559/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0163 - acc: 0.5390 - val_loss: 12.6410 - val_acc: 0.1100\n",
      "Epoch 560/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0141 - acc: 0.5520 - val_loss: 12.6500 - val_acc: 0.1200\n",
      "Epoch 561/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0159 - acc: 0.5210 - val_loss: 12.6410 - val_acc: 0.1200\n",
      "Epoch 562/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0180 - acc: 0.5370 - val_loss: 12.6617 - val_acc: 0.1300\n",
      "Epoch 563/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.0189 - acc: 0.5280 - val_loss: 12.6587 - val_acc: 0.1300\n",
      "Epoch 564/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0178 - acc: 0.5390 - val_loss: 12.6336 - val_acc: 0.1100\n",
      "Epoch 565/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0175 - acc: 0.5190 - val_loss: 12.6580 - val_acc: 0.1100\n",
      "Epoch 566/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0165 - acc: 0.5520 - val_loss: 12.6302 - val_acc: 0.1100\n",
      "Epoch 567/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0152 - acc: 0.5430 - val_loss: 12.6605 - val_acc: 0.1200\n",
      "Epoch 568/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0166 - acc: 0.5200 - val_loss: 12.6301 - val_acc: 0.1300\n",
      "Epoch 569/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0173 - acc: 0.5210 - val_loss: 12.6478 - val_acc: 0.1200\n",
      "Epoch 570/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0161 - acc: 0.5240 - val_loss: 12.6597 - val_acc: 0.1200\n",
      "Epoch 571/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0167 - acc: 0.5400 - val_loss: 12.6558 - val_acc: 0.1300\n",
      "Epoch 572/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0132 - acc: 0.5300 - val_loss: 12.6388 - val_acc: 0.1400\n",
      "Epoch 573/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.0145 - acc: 0.5310 - val_loss: 12.6614 - val_acc: 0.1200\n",
      "Epoch 574/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.0138 - acc: 0.5170 - val_loss: 12.6666 - val_acc: 0.1000\n",
      "Epoch 575/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.0144 - acc: 0.5410 - val_loss: 12.6745 - val_acc: 0.1100\n",
      "Epoch 576/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0158 - acc: 0.5230 - val_loss: 12.6524 - val_acc: 0.1000\n",
      "Epoch 577/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0142 - acc: 0.5280 - val_loss: 12.6381 - val_acc: 0.1200\n",
      "Epoch 578/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0122 - acc: 0.5360 - val_loss: 12.6776 - val_acc: 0.1100\n",
      "Epoch 579/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0114 - acc: 0.5330 - val_loss: 12.6434 - val_acc: 0.1200\n",
      "Epoch 580/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.0140 - acc: 0.5230 - val_loss: 12.6705 - val_acc: 0.1100\n",
      "Epoch 581/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0112 - acc: 0.5390 - val_loss: 12.6806 - val_acc: 0.1000\n",
      "Epoch 582/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0127 - acc: 0.5390 - val_loss: 12.6895 - val_acc: 0.1100\n",
      "Epoch 583/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0135 - acc: 0.5450 - val_loss: 12.6850 - val_acc: 0.1000\n",
      "Epoch 584/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 11.0148 - acc: 0.5180 - val_loss: 12.6779 - val_acc: 0.1100\n",
      "Epoch 585/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0120 - acc: 0.5470 - val_loss: 12.6758 - val_acc: 0.1300\n",
      "Epoch 586/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.0093 - acc: 0.5290 - val_loss: 12.6782 - val_acc: 0.1100\n",
      "Epoch 587/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0099 - acc: 0.5320 - val_loss: 12.6964 - val_acc: 0.1300\n",
      "Epoch 588/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0133 - acc: 0.5320 - val_loss: 12.6783 - val_acc: 0.0900\n",
      "Epoch 589/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0114 - acc: 0.5270 - val_loss: 12.7075 - val_acc: 0.1000\n",
      "Epoch 590/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0131 - acc: 0.5330 - val_loss: 12.6789 - val_acc: 0.1000\n",
      "Epoch 591/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0109 - acc: 0.5310 - val_loss: 12.6919 - val_acc: 0.1100\n",
      "Epoch 592/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0100 - acc: 0.5550 - val_loss: 12.6974 - val_acc: 0.1300\n",
      "Epoch 593/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0092 - acc: 0.5370 - val_loss: 12.6920 - val_acc: 0.1100\n",
      "Epoch 594/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.0127 - acc: 0.5320 - val_loss: 12.7162 - val_acc: 0.1200\n",
      "Epoch 595/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0126 - acc: 0.5470 - val_loss: 12.6862 - val_acc: 0.1100\n",
      "Epoch 596/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0097 - acc: 0.5420 - val_loss: 12.6968 - val_acc: 0.0900\n",
      "Epoch 597/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0105 - acc: 0.5440 - val_loss: 12.6984 - val_acc: 0.1000\n",
      "Epoch 598/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0117 - acc: 0.5270 - val_loss: 12.7068 - val_acc: 0.1300\n",
      "Epoch 599/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0088 - acc: 0.5520 - val_loss: 12.6962 - val_acc: 0.0800\n",
      "Epoch 600/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0106 - acc: 0.5200 - val_loss: 12.7045 - val_acc: 0.0800\n",
      "Epoch 601/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0091 - acc: 0.5540 - val_loss: 12.7125 - val_acc: 0.1300\n",
      "Epoch 602/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.0072 - acc: 0.5380 - val_loss: 12.7292 - val_acc: 0.0900\n",
      "Epoch 603/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0075 - acc: 0.5430 - val_loss: 12.7055 - val_acc: 0.1000\n",
      "Epoch 604/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0081 - acc: 0.5500 - val_loss: 12.7191 - val_acc: 0.1100\n",
      "Epoch 605/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0071 - acc: 0.5490 - val_loss: 12.7194 - val_acc: 0.0900\n",
      "Epoch 606/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0064 - acc: 0.5400 - val_loss: 12.7065 - val_acc: 0.1000\n",
      "Epoch 607/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 11.0080 - acc: 0.5310 - val_loss: 12.7155 - val_acc: 0.1200\n",
      "Epoch 608/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0092 - acc: 0.5280 - val_loss: 12.7117 - val_acc: 0.0800\n",
      "Epoch 609/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0068 - acc: 0.5400 - val_loss: 12.7212 - val_acc: 0.0900\n",
      "Epoch 610/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.0057 - acc: 0.5450 - val_loss: 12.7119 - val_acc: 0.1200\n",
      "Epoch 611/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0062 - acc: 0.5330 - val_loss: 12.7222 - val_acc: 0.0900\n",
      "Epoch 612/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0068 - acc: 0.5460 - val_loss: 12.7147 - val_acc: 0.1000\n",
      "Epoch 613/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0052 - acc: 0.5440 - val_loss: 12.7548 - val_acc: 0.0900\n",
      "Epoch 614/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0069 - acc: 0.5500 - val_loss: 12.7258 - val_acc: 0.1100\n",
      "Epoch 615/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0056 - acc: 0.5440 - val_loss: 12.7254 - val_acc: 0.1200\n",
      "Epoch 616/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0056 - acc: 0.5360 - val_loss: 12.7294 - val_acc: 0.1100\n",
      "Epoch 617/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.0065 - acc: 0.5370 - val_loss: 12.7312 - val_acc: 0.0800\n",
      "Epoch 618/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0089 - acc: 0.5350 - val_loss: 12.7200 - val_acc: 0.1000\n",
      "Epoch 619/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0069 - acc: 0.5340 - val_loss: 12.7387 - val_acc: 0.1100\n",
      "Epoch 620/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0035 - acc: 0.5410 - val_loss: 12.7454 - val_acc: 0.1200\n",
      "Epoch 621/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0047 - acc: 0.5570 - val_loss: 12.7371 - val_acc: 0.1100\n",
      "Epoch 622/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0062 - acc: 0.5450 - val_loss: 12.7254 - val_acc: 0.1000\n",
      "Epoch 623/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0027 - acc: 0.5290 - val_loss: 12.7548 - val_acc: 0.0800\n",
      "Epoch 624/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0034 - acc: 0.5410 - val_loss: 12.7327 - val_acc: 0.0900\n",
      "Epoch 625/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0054 - acc: 0.5350 - val_loss: 12.7203 - val_acc: 0.1200\n",
      "Epoch 626/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 11.0041 - acc: 0.5400 - val_loss: 12.7335 - val_acc: 0.1200\n",
      "Epoch 627/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0053 - acc: 0.5390 - val_loss: 12.7460 - val_acc: 0.1000\n",
      "Epoch 628/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0035 - acc: 0.5470 - val_loss: 12.7753 - val_acc: 0.1100\n",
      "Epoch 629/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0051 - acc: 0.5500 - val_loss: 12.7444 - val_acc: 0.1100\n",
      "Epoch 630/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0056 - acc: 0.5400 - val_loss: 12.7493 - val_acc: 0.1100\n",
      "Epoch 631/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0033 - acc: 0.5440 - val_loss: 12.7541 - val_acc: 0.0900\n",
      "Epoch 632/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0054 - acc: 0.5280 - val_loss: 12.7492 - val_acc: 0.1000\n",
      "Epoch 633/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0062 - acc: 0.5480 - val_loss: 12.7548 - val_acc: 0.0900\n",
      "Epoch 634/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0047 - acc: 0.5320 - val_loss: 12.7325 - val_acc: 0.0900\n",
      "Epoch 635/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0016 - acc: 0.5480 - val_loss: 12.7671 - val_acc: 0.1000\n",
      "Epoch 636/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.0052 - acc: 0.5380 - val_loss: 12.7670 - val_acc: 0.0800\n",
      "Epoch 637/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0010 - acc: 0.5530 - val_loss: 12.7461 - val_acc: 0.0800\n",
      "Epoch 638/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0016 - acc: 0.5340 - val_loss: 12.7462 - val_acc: 0.1000\n",
      "Epoch 639/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 44us/step - loss: 11.0007 - acc: 0.5600 - val_loss: 12.7445 - val_acc: 0.0800\n",
      "Epoch 640/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0065 - acc: 0.5480 - val_loss: 12.7609 - val_acc: 0.1200\n",
      "Epoch 641/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0031 - acc: 0.5520 - val_loss: 12.7641 - val_acc: 0.0800\n",
      "Epoch 642/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0023 - acc: 0.5500 - val_loss: 12.7577 - val_acc: 0.1100\n",
      "Epoch 643/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0024 - acc: 0.5260 - val_loss: 12.7750 - val_acc: 0.0800\n",
      "Epoch 644/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0012 - acc: 0.5420 - val_loss: 12.7678 - val_acc: 0.0900\n",
      "Epoch 645/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0023 - acc: 0.5550 - val_loss: 12.7815 - val_acc: 0.1200\n",
      "Epoch 646/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 11.0001 - acc: 0.5460 - val_loss: 12.7816 - val_acc: 0.0900\n",
      "Epoch 647/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0003 - acc: 0.5520 - val_loss: 12.7720 - val_acc: 0.1200\n",
      "Epoch 648/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0003 - acc: 0.5500 - val_loss: 12.7713 - val_acc: 0.1000\n",
      "Epoch 649/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0005 - acc: 0.5610 - val_loss: 12.7769 - val_acc: 0.1000\n",
      "Epoch 650/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9998 - acc: 0.5430 - val_loss: 12.7782 - val_acc: 0.0800\n",
      "Epoch 651/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9982 - acc: 0.5510 - val_loss: 12.7741 - val_acc: 0.0900\n",
      "Epoch 652/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9986 - acc: 0.5510 - val_loss: 12.7783 - val_acc: 0.1000\n",
      "Epoch 653/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0003 - acc: 0.5440 - val_loss: 12.7775 - val_acc: 0.0900\n",
      "Epoch 654/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0008 - acc: 0.5500 - val_loss: 12.7819 - val_acc: 0.1000\n",
      "Epoch 655/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0018 - acc: 0.5460 - val_loss: 12.8227 - val_acc: 0.1200\n",
      "Epoch 656/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9995 - acc: 0.5560 - val_loss: 12.7811 - val_acc: 0.1000\n",
      "Epoch 657/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.0007 - acc: 0.5590 - val_loss: 12.7778 - val_acc: 0.1100\n",
      "Epoch 658/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9995 - acc: 0.5470 - val_loss: 12.7976 - val_acc: 0.1200\n",
      "Epoch 659/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9996 - acc: 0.5500 - val_loss: 12.7653 - val_acc: 0.1000\n",
      "Epoch 660/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9983 - acc: 0.5520 - val_loss: 12.7811 - val_acc: 0.0800\n",
      "Epoch 661/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9992 - acc: 0.5410 - val_loss: 12.7944 - val_acc: 0.0800\n",
      "Epoch 662/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9993 - acc: 0.5490 - val_loss: 12.8204 - val_acc: 0.1200\n",
      "Epoch 663/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0011 - acc: 0.5460 - val_loss: 12.7902 - val_acc: 0.0900\n",
      "Epoch 664/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.0002 - acc: 0.5450 - val_loss: 12.7924 - val_acc: 0.0900\n",
      "Epoch 665/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.0007 - acc: 0.5340 - val_loss: 12.7807 - val_acc: 0.0900\n",
      "Epoch 666/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9998 - acc: 0.5630 - val_loss: 12.7953 - val_acc: 0.0900\n",
      "Epoch 667/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9995 - acc: 0.5560 - val_loss: 12.7992 - val_acc: 0.1100\n",
      "Epoch 668/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9992 - acc: 0.5490 - val_loss: 12.8251 - val_acc: 0.0900\n",
      "Epoch 669/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9991 - acc: 0.5400 - val_loss: 12.8056 - val_acc: 0.0900\n",
      "Epoch 670/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9975 - acc: 0.5480 - val_loss: 12.8087 - val_acc: 0.0800\n",
      "Epoch 671/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9981 - acc: 0.5670 - val_loss: 12.8218 - val_acc: 0.0900\n",
      "Epoch 672/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9980 - acc: 0.5530 - val_loss: 12.8142 - val_acc: 0.0900\n",
      "Epoch 673/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9984 - acc: 0.5590 - val_loss: 12.8119 - val_acc: 0.0900\n",
      "Epoch 674/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9969 - acc: 0.5520 - val_loss: 12.7937 - val_acc: 0.0900\n",
      "Epoch 675/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9972 - acc: 0.5460 - val_loss: 12.8142 - val_acc: 0.1000\n",
      "Epoch 676/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9977 - acc: 0.5420 - val_loss: 12.8090 - val_acc: 0.1200\n",
      "Epoch 677/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9969 - acc: 0.5440 - val_loss: 12.8117 - val_acc: 0.1200\n",
      "Epoch 678/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9964 - acc: 0.5390 - val_loss: 12.8061 - val_acc: 0.0800\n",
      "Epoch 679/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9982 - acc: 0.5480 - val_loss: 12.8018 - val_acc: 0.1200\n",
      "Epoch 680/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9979 - acc: 0.5470 - val_loss: 12.8171 - val_acc: 0.1000\n",
      "Epoch 681/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9985 - acc: 0.5490 - val_loss: 12.8000 - val_acc: 0.0800\n",
      "Epoch 682/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9986 - acc: 0.5470 - val_loss: 12.8410 - val_acc: 0.1100\n",
      "Epoch 683/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9958 - acc: 0.5530 - val_loss: 12.8208 - val_acc: 0.1100\n",
      "Epoch 684/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9973 - acc: 0.5330 - val_loss: 12.8289 - val_acc: 0.1000\n",
      "Epoch 685/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9981 - acc: 0.5410 - val_loss: 12.8277 - val_acc: 0.0900\n",
      "Epoch 686/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9968 - acc: 0.5470 - val_loss: 12.8357 - val_acc: 0.1000\n",
      "Epoch 687/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9966 - acc: 0.5490 - val_loss: 12.8380 - val_acc: 0.1000\n",
      "Epoch 688/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9976 - acc: 0.5310 - val_loss: 12.8106 - val_acc: 0.1100\n",
      "Epoch 689/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9934 - acc: 0.5540 - val_loss: 12.8203 - val_acc: 0.0900\n",
      "Epoch 690/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9961 - acc: 0.5510 - val_loss: 12.8490 - val_acc: 0.1000\n",
      "Epoch 691/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9935 - acc: 0.5540 - val_loss: 12.8070 - val_acc: 0.1000\n",
      "Epoch 692/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9943 - acc: 0.5320 - val_loss: 12.8315 - val_acc: 0.1000\n",
      "Epoch 693/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9952 - acc: 0.5300 - val_loss: 12.8595 - val_acc: 0.0900\n",
      "Epoch 694/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9987 - acc: 0.5520 - val_loss: 12.8413 - val_acc: 0.1100\n",
      "Epoch 695/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9958 - acc: 0.5660 - val_loss: 12.8398 - val_acc: 0.1000\n",
      "Epoch 696/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9943 - acc: 0.5550 - val_loss: 12.8430 - val_acc: 0.1200\n",
      "Epoch 697/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9962 - acc: 0.5400 - val_loss: 12.8467 - val_acc: 0.1100\n",
      "Epoch 698/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9918 - acc: 0.5510 - val_loss: 12.8399 - val_acc: 0.1100\n",
      "Epoch 699/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9935 - acc: 0.5640 - val_loss: 12.8512 - val_acc: 0.1200\n",
      "Epoch 700/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9946 - acc: 0.5550 - val_loss: 12.8428 - val_acc: 0.1300\n",
      "Epoch 701/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9958 - acc: 0.5520 - val_loss: 12.8530 - val_acc: 0.1000\n",
      "Epoch 702/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9943 - acc: 0.5560 - val_loss: 12.8653 - val_acc: 0.1100\n",
      "Epoch 703/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9951 - acc: 0.5250 - val_loss: 12.8469 - val_acc: 0.1100\n",
      "Epoch 704/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9931 - acc: 0.5470 - val_loss: 12.8703 - val_acc: 0.0900\n",
      "Epoch 705/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9959 - acc: 0.5450 - val_loss: 12.8451 - val_acc: 0.0800\n",
      "Epoch 706/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9935 - acc: 0.5440 - val_loss: 12.8504 - val_acc: 0.1200\n",
      "Epoch 707/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9922 - acc: 0.5320 - val_loss: 12.8569 - val_acc: 0.1000\n",
      "Epoch 708/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9935 - acc: 0.5450 - val_loss: 12.8517 - val_acc: 0.1100\n",
      "Epoch 709/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9923 - acc: 0.5530 - val_loss: 12.8635 - val_acc: 0.1200\n",
      "Epoch 710/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9937 - acc: 0.5380 - val_loss: 12.8542 - val_acc: 0.1100\n",
      "Epoch 711/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9935 - acc: 0.5420 - val_loss: 12.8692 - val_acc: 0.0900\n",
      "Epoch 712/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9911 - acc: 0.5550 - val_loss: 12.8668 - val_acc: 0.0900\n",
      "Epoch 713/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9930 - acc: 0.5470 - val_loss: 12.8769 - val_acc: 0.1100\n",
      "Epoch 714/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9916 - acc: 0.5440 - val_loss: 12.8445 - val_acc: 0.1000\n",
      "Epoch 715/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9918 - acc: 0.5520 - val_loss: 12.8706 - val_acc: 0.1300\n",
      "Epoch 716/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9911 - acc: 0.5520 - val_loss: 12.8615 - val_acc: 0.1100\n",
      "Epoch 717/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9920 - acc: 0.5600 - val_loss: 12.8497 - val_acc: 0.0700\n",
      "Epoch 718/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9938 - acc: 0.5320 - val_loss: 12.8595 - val_acc: 0.1100\n",
      "Epoch 719/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9913 - acc: 0.5530 - val_loss: 12.8810 - val_acc: 0.1300\n",
      "Epoch 720/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9906 - acc: 0.5450 - val_loss: 12.8866 - val_acc: 0.1000\n",
      "Epoch 721/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9892 - acc: 0.5360 - val_loss: 12.8849 - val_acc: 0.1100\n",
      "Epoch 722/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9895 - acc: 0.5500 - val_loss: 12.8729 - val_acc: 0.1100\n",
      "Epoch 723/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9916 - acc: 0.5540 - val_loss: 12.8880 - val_acc: 0.0800\n",
      "Epoch 724/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9879 - acc: 0.5810 - val_loss: 12.8823 - val_acc: 0.1000\n",
      "Epoch 725/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9909 - acc: 0.5540 - val_loss: 12.8832 - val_acc: 0.1200\n",
      "Epoch 726/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 10.9936 - acc: 0.5330 - val_loss: 12.8962 - val_acc: 0.0800\n",
      "Epoch 727/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9918 - acc: 0.5490 - val_loss: 12.8653 - val_acc: 0.1300\n",
      "Epoch 728/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9887 - acc: 0.5570 - val_loss: 12.8901 - val_acc: 0.1100\n",
      "Epoch 729/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 10.9899 - acc: 0.5320 - val_loss: 12.8961 - val_acc: 0.1100\n",
      "Epoch 730/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9898 - acc: 0.5460 - val_loss: 12.8847 - val_acc: 0.0800\n",
      "Epoch 731/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9900 - acc: 0.5640 - val_loss: 12.9138 - val_acc: 0.1200\n",
      "Epoch 732/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9895 - acc: 0.5520 - val_loss: 12.8928 - val_acc: 0.1200\n",
      "Epoch 733/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9879 - acc: 0.5570 - val_loss: 12.9132 - val_acc: 0.0800\n",
      "Epoch 734/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9875 - acc: 0.5710 - val_loss: 12.8983 - val_acc: 0.1000\n",
      "Epoch 735/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9872 - acc: 0.5630 - val_loss: 12.9123 - val_acc: 0.1100\n",
      "Epoch 736/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9890 - acc: 0.5540 - val_loss: 12.9125 - val_acc: 0.1000\n",
      "Epoch 737/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9901 - acc: 0.5430 - val_loss: 12.8769 - val_acc: 0.1000\n",
      "Epoch 738/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9920 - acc: 0.5550 - val_loss: 12.9119 - val_acc: 0.1100\n",
      "Epoch 739/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9900 - acc: 0.5500 - val_loss: 12.9218 - val_acc: 0.1100\n",
      "Epoch 740/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9891 - acc: 0.5470 - val_loss: 12.9040 - val_acc: 0.1200\n",
      "Epoch 741/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 10.9888 - acc: 0.5520 - val_loss: 12.9116 - val_acc: 0.0800\n",
      "Epoch 742/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9880 - acc: 0.5600 - val_loss: 12.9238 - val_acc: 0.1000\n",
      "Epoch 743/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 10.9858 - acc: 0.5580 - val_loss: 12.9201 - val_acc: 0.0700\n",
      "Epoch 744/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9895 - acc: 0.5380 - val_loss: 12.8986 - val_acc: 0.1000\n",
      "Epoch 745/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9889 - acc: 0.5490 - val_loss: 12.9010 - val_acc: 0.1200\n",
      "Epoch 746/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9905 - acc: 0.5420 - val_loss: 12.9180 - val_acc: 0.1200\n",
      "Epoch 747/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9891 - acc: 0.5470 - val_loss: 12.8996 - val_acc: 0.1000\n",
      "Epoch 748/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9878 - acc: 0.5510 - val_loss: 12.9360 - val_acc: 0.1100\n",
      "Epoch 749/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9890 - acc: 0.5430 - val_loss: 12.9263 - val_acc: 0.0800\n",
      "Epoch 750/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9880 - acc: 0.5500 - val_loss: 12.9260 - val_acc: 0.0900\n",
      "Epoch 751/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9893 - acc: 0.5560 - val_loss: 12.9233 - val_acc: 0.1100\n",
      "Epoch 752/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9869 - acc: 0.5620 - val_loss: 12.9280 - val_acc: 0.1200\n",
      "Epoch 753/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9882 - acc: 0.5490 - val_loss: 12.9282 - val_acc: 0.1200\n",
      "Epoch 754/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9850 - acc: 0.5520 - val_loss: 12.9275 - val_acc: 0.1000\n",
      "Epoch 755/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 49us/step - loss: 10.9880 - acc: 0.5630 - val_loss: 12.9179 - val_acc: 0.0800\n",
      "Epoch 756/1000\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 10.9862 - acc: 0.5580 - val_loss: 12.9158 - val_acc: 0.1200\n",
      "Epoch 757/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9864 - acc: 0.5670 - val_loss: 12.9288 - val_acc: 0.1100\n",
      "Epoch 758/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9869 - acc: 0.5460 - val_loss: 12.9304 - val_acc: 0.1000\n",
      "Epoch 759/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9861 - acc: 0.5530 - val_loss: 12.9137 - val_acc: 0.1200\n",
      "Epoch 760/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9859 - acc: 0.5740 - val_loss: 12.9399 - val_acc: 0.1000\n",
      "Epoch 761/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9883 - acc: 0.5440 - val_loss: 12.9227 - val_acc: 0.1000\n",
      "Epoch 762/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9858 - acc: 0.5600 - val_loss: 12.9302 - val_acc: 0.0900\n",
      "Epoch 763/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9842 - acc: 0.5480 - val_loss: 12.9191 - val_acc: 0.1100\n",
      "Epoch 764/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9877 - acc: 0.5450 - val_loss: 12.9003 - val_acc: 0.1100\n",
      "Epoch 765/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9861 - acc: 0.5460 - val_loss: 12.9468 - val_acc: 0.1200\n",
      "Epoch 766/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9889 - acc: 0.5480 - val_loss: 12.9485 - val_acc: 0.1100\n",
      "Epoch 767/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9829 - acc: 0.5600 - val_loss: 12.9349 - val_acc: 0.1200\n",
      "Epoch 768/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9835 - acc: 0.5460 - val_loss: 12.9561 - val_acc: 0.1000\n",
      "Epoch 769/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 10.9840 - acc: 0.5470 - val_loss: 12.9544 - val_acc: 0.1000\n",
      "Epoch 770/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9866 - acc: 0.5380 - val_loss: 12.9106 - val_acc: 0.1100\n",
      "Epoch 771/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9859 - acc: 0.5550 - val_loss: 12.9462 - val_acc: 0.1000\n",
      "Epoch 772/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9865 - acc: 0.5580 - val_loss: 12.9387 - val_acc: 0.1100\n",
      "Epoch 773/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9860 - acc: 0.5560 - val_loss: 12.9321 - val_acc: 0.1400\n",
      "Epoch 774/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9844 - acc: 0.5650 - val_loss: 12.9589 - val_acc: 0.0900\n",
      "Epoch 775/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 10.9857 - acc: 0.5430 - val_loss: 12.9324 - val_acc: 0.0800\n",
      "Epoch 776/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 10.9837 - acc: 0.5730 - val_loss: 12.9763 - val_acc: 0.1000\n",
      "Epoch 777/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9854 - acc: 0.5570 - val_loss: 12.9523 - val_acc: 0.1000\n",
      "Epoch 778/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9838 - acc: 0.5600 - val_loss: 12.9598 - val_acc: 0.1300\n",
      "Epoch 779/1000\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 10.9830 - acc: 0.5650 - val_loss: 12.9391 - val_acc: 0.1000\n",
      "Epoch 780/1000\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 10.9844 - acc: 0.5440 - val_loss: 12.9411 - val_acc: 0.1000\n",
      "Epoch 781/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 10.9833 - acc: 0.5450 - val_loss: 12.9481 - val_acc: 0.0900\n",
      "Epoch 782/1000\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 10.9849 - acc: 0.5380 - val_loss: 12.9541 - val_acc: 0.1200\n",
      "Epoch 783/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 10.9860 - acc: 0.5600 - val_loss: 12.9468 - val_acc: 0.1200\n",
      "Epoch 784/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 10.9851 - acc: 0.5550 - val_loss: 12.9749 - val_acc: 0.0800\n",
      "Epoch 785/1000\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 10.9825 - acc: 0.5500 - val_loss: 12.9693 - val_acc: 0.1000\n",
      "Epoch 786/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 10.9838 - acc: 0.5580 - val_loss: 12.9587 - val_acc: 0.1000\n",
      "Epoch 787/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9830 - acc: 0.5480 - val_loss: 12.9537 - val_acc: 0.0900\n",
      "Epoch 788/1000\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 10.9813 - acc: 0.5620 - val_loss: 12.9733 - val_acc: 0.1100\n",
      "Epoch 789/1000\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 10.9841 - acc: 0.5490 - val_loss: 12.9531 - val_acc: 0.1100\n",
      "Epoch 790/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9844 - acc: 0.5560 - val_loss: 12.9733 - val_acc: 0.1400\n",
      "Epoch 791/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9843 - acc: 0.5740 - val_loss: 12.9394 - val_acc: 0.1000\n",
      "Epoch 792/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 10.9846 - acc: 0.5670 - val_loss: 12.9482 - val_acc: 0.1100\n",
      "Epoch 793/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9821 - acc: 0.5670 - val_loss: 12.9642 - val_acc: 0.1100\n",
      "Epoch 794/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9821 - acc: 0.5290 - val_loss: 12.9898 - val_acc: 0.1000\n",
      "Epoch 795/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9822 - acc: 0.5360 - val_loss: 12.9797 - val_acc: 0.0800\n",
      "Epoch 796/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9828 - acc: 0.5550 - val_loss: 12.9922 - val_acc: 0.1300\n",
      "Epoch 797/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 10.9850 - acc: 0.5690 - val_loss: 12.9894 - val_acc: 0.0800\n",
      "Epoch 798/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9823 - acc: 0.5430 - val_loss: 12.9943 - val_acc: 0.0900\n",
      "Epoch 799/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 10.9060 - acc: 0.50 - 0s 50us/step - loss: 10.9823 - acc: 0.5440 - val_loss: 12.9817 - val_acc: 0.1200\n",
      "Epoch 800/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 10.9831 - acc: 0.5550 - val_loss: 12.9769 - val_acc: 0.1200\n",
      "Epoch 801/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9824 - acc: 0.5430 - val_loss: 12.9496 - val_acc: 0.1100\n",
      "Epoch 802/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 10.9824 - acc: 0.5580 - val_loss: 12.9788 - val_acc: 0.1100\n",
      "Epoch 803/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9858 - acc: 0.5410 - val_loss: 12.9767 - val_acc: 0.1200\n",
      "Epoch 804/1000\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 10.9824 - acc: 0.5370 - val_loss: 12.9649 - val_acc: 0.1200\n",
      "Epoch 805/1000\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 10.9800 - acc: 0.5600 - val_loss: 12.9852 - val_acc: 0.1200\n",
      "Epoch 806/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9828 - acc: 0.5560 - val_loss: 12.9648 - val_acc: 0.1000\n",
      "Epoch 807/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 10.9799 - acc: 0.5550 - val_loss: 12.9921 - val_acc: 0.0900\n",
      "Epoch 808/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9806 - acc: 0.5470 - val_loss: 12.9932 - val_acc: 0.1000\n",
      "Epoch 809/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9800 - acc: 0.5610 - val_loss: 12.9715 - val_acc: 0.1300\n",
      "Epoch 810/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9811 - acc: 0.5680 - val_loss: 12.9939 - val_acc: 0.1100\n",
      "Epoch 811/1000\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 10.9793 - acc: 0.5500 - val_loss: 12.9985 - val_acc: 0.1200\n",
      "Epoch 812/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 11.2387 - acc: 0.50 - 0s 49us/step - loss: 10.9810 - acc: 0.5430 - val_loss: 12.9896 - val_acc: 0.1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 813/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 11.0774 - acc: 0.65 - 0s 45us/step - loss: 10.9793 - acc: 0.5680 - val_loss: 12.9930 - val_acc: 0.1200\n",
      "Epoch 814/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 10.9812 - acc: 0.5490 - val_loss: 13.0073 - val_acc: 0.1000\n",
      "Epoch 815/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9801 - acc: 0.5470 - val_loss: 12.9891 - val_acc: 0.1000\n",
      "Epoch 816/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9817 - acc: 0.5660 - val_loss: 12.9915 - val_acc: 0.1000\n",
      "Epoch 817/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9796 - acc: 0.5560 - val_loss: 12.9993 - val_acc: 0.1000\n",
      "Epoch 818/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9808 - acc: 0.5510 - val_loss: 12.9909 - val_acc: 0.1300\n",
      "Epoch 819/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9821 - acc: 0.5580 - val_loss: 12.9997 - val_acc: 0.1000\n",
      "Epoch 820/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9811 - acc: 0.5560 - val_loss: 12.9760 - val_acc: 0.1100\n",
      "Epoch 821/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9810 - acc: 0.5660 - val_loss: 13.0147 - val_acc: 0.1000\n",
      "Epoch 822/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9789 - acc: 0.5520 - val_loss: 12.9862 - val_acc: 0.1100\n",
      "Epoch 823/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9806 - acc: 0.5620 - val_loss: 13.0153 - val_acc: 0.1100\n",
      "Epoch 824/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9789 - acc: 0.5600 - val_loss: 12.9768 - val_acc: 0.1000\n",
      "Epoch 825/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9774 - acc: 0.5700 - val_loss: 12.9935 - val_acc: 0.1100\n",
      "Epoch 826/1000\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 10.9789 - acc: 0.5620 - val_loss: 12.9939 - val_acc: 0.1400\n",
      "Epoch 827/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9797 - acc: 0.5440 - val_loss: 13.0079 - val_acc: 0.1100\n",
      "Epoch 828/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 10.9779 - acc: 0.5440 - val_loss: 13.0166 - val_acc: 0.1100\n",
      "Epoch 829/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9800 - acc: 0.5530 - val_loss: 12.9977 - val_acc: 0.1300\n",
      "Epoch 830/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 10.9813 - acc: 0.5430 - val_loss: 13.0211 - val_acc: 0.1100\n",
      "Epoch 831/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9810 - acc: 0.5630 - val_loss: 13.0071 - val_acc: 0.1000\n",
      "Epoch 832/1000\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 10.9786 - acc: 0.5690 - val_loss: 13.0017 - val_acc: 0.1200\n",
      "Epoch 833/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9785 - acc: 0.5600 - val_loss: 13.0057 - val_acc: 0.1300\n",
      "Epoch 834/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 10.9773 - acc: 0.5570 - val_loss: 13.0261 - val_acc: 0.1200\n",
      "Epoch 835/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9782 - acc: 0.5500 - val_loss: 13.0170 - val_acc: 0.0900\n",
      "Epoch 836/1000\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 10.9795 - acc: 0.5640 - val_loss: 13.0091 - val_acc: 0.1100\n",
      "Epoch 837/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 10.1080 - acc: 0.59 - 0s 43us/step - loss: 10.9769 - acc: 0.5720 - val_loss: 13.0159 - val_acc: 0.1100\n",
      "Epoch 838/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9834 - acc: 0.5470 - val_loss: 13.0147 - val_acc: 0.1100\n",
      "Epoch 839/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 10.9784 - acc: 0.5620 - val_loss: 13.0153 - val_acc: 0.1100\n",
      "Epoch 840/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9788 - acc: 0.5480 - val_loss: 13.0394 - val_acc: 0.1100\n",
      "Epoch 841/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9807 - acc: 0.5530 - val_loss: 13.0118 - val_acc: 0.1200\n",
      "Epoch 842/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 10.9781 - acc: 0.5470 - val_loss: 13.0067 - val_acc: 0.1100\n",
      "Epoch 843/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9795 - acc: 0.5710 - val_loss: 13.0150 - val_acc: 0.0900\n",
      "Epoch 844/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 10.9797 - acc: 0.5650 - val_loss: 13.0274 - val_acc: 0.1300\n",
      "Epoch 845/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9804 - acc: 0.5490 - val_loss: 13.0137 - val_acc: 0.1100\n",
      "Epoch 846/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9791 - acc: 0.5490 - val_loss: 13.0256 - val_acc: 0.1100\n",
      "Epoch 847/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9791 - acc: 0.5600 - val_loss: 13.0286 - val_acc: 0.1100\n",
      "Epoch 848/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9773 - acc: 0.5410 - val_loss: 13.0194 - val_acc: 0.0900\n",
      "Epoch 849/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 10.9794 - acc: 0.5750 - val_loss: 13.0429 - val_acc: 0.1100\n",
      "Epoch 850/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9801 - acc: 0.5410 - val_loss: 13.0491 - val_acc: 0.1100\n",
      "Epoch 851/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9789 - acc: 0.5490 - val_loss: 13.0322 - val_acc: 0.1000\n",
      "Epoch 852/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9772 - acc: 0.5570 - val_loss: 13.0132 - val_acc: 0.1200\n",
      "Epoch 853/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9780 - acc: 0.5680 - val_loss: 13.0166 - val_acc: 0.1100\n",
      "Epoch 854/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9769 - acc: 0.5650 - val_loss: 13.0146 - val_acc: 0.1100\n",
      "Epoch 855/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 10.9773 - acc: 0.5550 - val_loss: 13.0326 - val_acc: 0.1000\n",
      "Epoch 856/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 10.9771 - acc: 0.5510 - val_loss: 13.0337 - val_acc: 0.1200\n",
      "Epoch 857/1000\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 10.9763 - acc: 0.5720 - val_loss: 13.0425 - val_acc: 0.1100\n",
      "Epoch 858/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 11.1293 - acc: 0.56 - 0s 44us/step - loss: 10.9762 - acc: 0.5500 - val_loss: 13.0560 - val_acc: 0.1200\n",
      "Epoch 859/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9774 - acc: 0.5650 - val_loss: 13.0231 - val_acc: 0.1100\n",
      "Epoch 860/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9775 - acc: 0.5550 - val_loss: 13.0401 - val_acc: 0.1300\n",
      "Epoch 861/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 10.9782 - acc: 0.5480 - val_loss: 13.0377 - val_acc: 0.1300\n",
      "Epoch 862/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9778 - acc: 0.5300 - val_loss: 13.0533 - val_acc: 0.1100\n",
      "Epoch 863/1000\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 10.9801 - acc: 0.5610 - val_loss: 13.0236 - val_acc: 0.1400\n",
      "Epoch 864/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9784 - acc: 0.5440 - val_loss: 13.0432 - val_acc: 0.0900\n",
      "Epoch 865/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9749 - acc: 0.5540 - val_loss: 13.0406 - val_acc: 0.1100\n",
      "Epoch 866/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9741 - acc: 0.5550 - val_loss: 13.0594 - val_acc: 0.1300\n",
      "Epoch 867/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9779 - acc: 0.5680 - val_loss: 13.0543 - val_acc: 0.1100\n",
      "Epoch 868/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9756 - acc: 0.5460 - val_loss: 13.0553 - val_acc: 0.1100\n",
      "Epoch 869/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 10.9762 - acc: 0.5650 - val_loss: 13.0643 - val_acc: 0.1200\n",
      "Epoch 870/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9773 - acc: 0.5760 - val_loss: 13.0554 - val_acc: 0.1100\n",
      "Epoch 871/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9744 - acc: 0.5500 - val_loss: 13.0647 - val_acc: 0.1000\n",
      "Epoch 872/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9778 - acc: 0.5340 - val_loss: 13.0690 - val_acc: 0.1000\n",
      "Epoch 873/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9762 - acc: 0.5360 - val_loss: 13.0557 - val_acc: 0.1100\n",
      "Epoch 874/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9751 - acc: 0.5600 - val_loss: 13.0288 - val_acc: 0.1100\n",
      "Epoch 875/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9760 - acc: 0.5510 - val_loss: 13.0643 - val_acc: 0.1300\n",
      "Epoch 876/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9745 - acc: 0.5480 - val_loss: 13.0727 - val_acc: 0.1200\n",
      "Epoch 877/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9778 - acc: 0.5610 - val_loss: 13.0603 - val_acc: 0.1200\n",
      "Epoch 878/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9746 - acc: 0.5590 - val_loss: 13.0552 - val_acc: 0.1000\n",
      "Epoch 879/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9756 - acc: 0.5680 - val_loss: 13.0499 - val_acc: 0.1200\n",
      "Epoch 880/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9741 - acc: 0.5400 - val_loss: 13.0419 - val_acc: 0.1200\n",
      "Epoch 881/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9755 - acc: 0.5460 - val_loss: 13.0441 - val_acc: 0.1000\n",
      "Epoch 882/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9770 - acc: 0.5370 - val_loss: 13.0596 - val_acc: 0.1200\n",
      "Epoch 883/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9754 - acc: 0.5440 - val_loss: 13.0707 - val_acc: 0.1100\n",
      "Epoch 884/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9758 - acc: 0.5580 - val_loss: 13.0573 - val_acc: 0.1200\n",
      "Epoch 885/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9750 - acc: 0.5660 - val_loss: 13.0612 - val_acc: 0.1300\n",
      "Epoch 886/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9745 - acc: 0.5530 - val_loss: 13.0447 - val_acc: 0.1100\n",
      "Epoch 887/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9721 - acc: 0.5670 - val_loss: 13.0464 - val_acc: 0.1200\n",
      "Epoch 888/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9764 - acc: 0.5400 - val_loss: 13.0750 - val_acc: 0.1300\n",
      "Epoch 889/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9759 - acc: 0.5540 - val_loss: 13.0503 - val_acc: 0.1300\n",
      "Epoch 890/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9741 - acc: 0.5410 - val_loss: 13.0842 - val_acc: 0.1200\n",
      "Epoch 891/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 10.9730 - acc: 0.5500 - val_loss: 13.0485 - val_acc: 0.1000\n",
      "Epoch 892/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9730 - acc: 0.5560 - val_loss: 13.0572 - val_acc: 0.1200\n",
      "Epoch 893/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9741 - acc: 0.5640 - val_loss: 13.0905 - val_acc: 0.1300\n",
      "Epoch 894/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9753 - acc: 0.5500 - val_loss: 13.0607 - val_acc: 0.1200\n",
      "Epoch 895/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9751 - acc: 0.5480 - val_loss: 13.0667 - val_acc: 0.1200\n",
      "Epoch 896/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9747 - acc: 0.5570 - val_loss: 13.0740 - val_acc: 0.1100\n",
      "Epoch 897/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9745 - acc: 0.5650 - val_loss: 13.1058 - val_acc: 0.1200\n",
      "Epoch 898/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 10.9767 - acc: 0.5360 - val_loss: 13.0631 - val_acc: 0.1300\n",
      "Epoch 899/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9753 - acc: 0.5390 - val_loss: 13.0919 - val_acc: 0.1100\n",
      "Epoch 900/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9745 - acc: 0.5620 - val_loss: 13.0858 - val_acc: 0.1100\n",
      "Epoch 901/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9732 - acc: 0.5540 - val_loss: 13.0815 - val_acc: 0.1300\n",
      "Epoch 902/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 10.9737 - acc: 0.5660 - val_loss: 13.0671 - val_acc: 0.1100\n",
      "Epoch 903/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9750 - acc: 0.5370 - val_loss: 13.0709 - val_acc: 0.1200\n",
      "Epoch 904/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9735 - acc: 0.5380 - val_loss: 13.0706 - val_acc: 0.1100\n",
      "Epoch 905/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9751 - acc: 0.5540 - val_loss: 13.0649 - val_acc: 0.1100\n",
      "Epoch 906/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 10.9762 - acc: 0.5320 - val_loss: 13.0360 - val_acc: 0.1100\n",
      "Epoch 907/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 10.9724 - acc: 0.5490 - val_loss: 13.0729 - val_acc: 0.0900\n",
      "Epoch 908/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 10.9743 - acc: 0.5490 - val_loss: 13.0685 - val_acc: 0.1100\n",
      "Epoch 909/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9716 - acc: 0.5690 - val_loss: 13.0709 - val_acc: 0.1100\n",
      "Epoch 910/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 10.9707 - acc: 0.5440 - val_loss: 13.0746 - val_acc: 0.0900\n",
      "Epoch 911/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 10.8144 - acc: 0.62 - 0s 45us/step - loss: 10.9734 - acc: 0.5540 - val_loss: 13.0803 - val_acc: 0.1100\n",
      "Epoch 912/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 10.9716 - acc: 0.5670 - val_loss: 13.0935 - val_acc: 0.1200\n",
      "Epoch 913/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 10.9733 - acc: 0.5580 - val_loss: 13.0617 - val_acc: 0.1100\n",
      "Epoch 914/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 10.9723 - acc: 0.5610 - val_loss: 13.0799 - val_acc: 0.1200\n",
      "Epoch 915/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9722 - acc: 0.5580 - val_loss: 13.0958 - val_acc: 0.1100\n",
      "Epoch 916/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 10.9705 - acc: 0.5510 - val_loss: 13.0931 - val_acc: 0.1200\n",
      "Epoch 917/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9715 - acc: 0.5510 - val_loss: 13.0915 - val_acc: 0.1100\n",
      "Epoch 918/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9703 - acc: 0.5650 - val_loss: 13.0819 - val_acc: 0.1200\n",
      "Epoch 919/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 10.9723 - acc: 0.5530 - val_loss: 13.0828 - val_acc: 0.1100\n",
      "Epoch 920/1000\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 10.9716 - acc: 0.5480 - val_loss: 13.1112 - val_acc: 0.1400\n",
      "Epoch 921/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9723 - acc: 0.5430 - val_loss: 13.1049 - val_acc: 0.1200\n",
      "Epoch 922/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9704 - acc: 0.5500 - val_loss: 13.1226 - val_acc: 0.1100\n",
      "Epoch 923/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 10.9730 - acc: 0.5440 - val_loss: 13.0900 - val_acc: 0.0900\n",
      "Epoch 924/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 10.9737 - acc: 0.5670 - val_loss: 13.1144 - val_acc: 0.1200\n",
      "Epoch 925/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9719 - acc: 0.5460 - val_loss: 13.1151 - val_acc: 0.1200\n",
      "Epoch 926/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9703 - acc: 0.5390 - val_loss: 13.1037 - val_acc: 0.1100\n",
      "Epoch 927/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9706 - acc: 0.5400 - val_loss: 13.1076 - val_acc: 0.1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 928/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9704 - acc: 0.5460 - val_loss: 13.1181 - val_acc: 0.1100\n",
      "Epoch 929/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9696 - acc: 0.5500 - val_loss: 13.1156 - val_acc: 0.1300\n",
      "Epoch 930/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9717 - acc: 0.5590 - val_loss: 13.0847 - val_acc: 0.1000\n",
      "Epoch 931/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9701 - acc: 0.5640 - val_loss: 13.1070 - val_acc: 0.1100\n",
      "Epoch 932/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9700 - acc: 0.5500 - val_loss: 13.1156 - val_acc: 0.1200\n",
      "Epoch 933/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9715 - acc: 0.5530 - val_loss: 13.1034 - val_acc: 0.1100\n",
      "Epoch 934/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9707 - acc: 0.5400 - val_loss: 13.1306 - val_acc: 0.1100\n",
      "Epoch 935/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9702 - acc: 0.5810 - val_loss: 13.1163 - val_acc: 0.1100\n",
      "Epoch 936/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9683 - acc: 0.5550 - val_loss: 13.1137 - val_acc: 0.1000\n",
      "Epoch 937/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 10.9693 - acc: 0.5540 - val_loss: 13.0949 - val_acc: 0.1200\n",
      "Epoch 938/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9699 - acc: 0.5540 - val_loss: 13.1085 - val_acc: 0.1400\n",
      "Epoch 939/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9686 - acc: 0.5610 - val_loss: 13.1311 - val_acc: 0.1200\n",
      "Epoch 940/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9692 - acc: 0.5470 - val_loss: 13.1131 - val_acc: 0.1000\n",
      "Epoch 941/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9707 - acc: 0.5520 - val_loss: 13.1046 - val_acc: 0.1200\n",
      "Epoch 942/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9713 - acc: 0.5520 - val_loss: 13.0841 - val_acc: 0.1000\n",
      "Epoch 943/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9708 - acc: 0.5540 - val_loss: 13.0991 - val_acc: 0.1100\n",
      "Epoch 944/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9689 - acc: 0.5660 - val_loss: 13.1129 - val_acc: 0.1200\n",
      "Epoch 945/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9711 - acc: 0.5490 - val_loss: 13.1086 - val_acc: 0.1200\n",
      "Epoch 946/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9710 - acc: 0.5610 - val_loss: 13.1282 - val_acc: 0.1200\n",
      "Epoch 947/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9718 - acc: 0.5600 - val_loss: 13.1190 - val_acc: 0.1200\n",
      "Epoch 948/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9757 - acc: 0.5480 - val_loss: 13.1342 - val_acc: 0.1000\n",
      "Epoch 949/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9735 - acc: 0.5360 - val_loss: 13.1266 - val_acc: 0.1200\n",
      "Epoch 950/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9709 - acc: 0.5520 - val_loss: 13.1697 - val_acc: 0.1100\n",
      "Epoch 951/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9697 - acc: 0.5600 - val_loss: 13.1194 - val_acc: 0.1200\n",
      "Epoch 952/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9683 - acc: 0.5510 - val_loss: 13.1393 - val_acc: 0.1300\n",
      "Epoch 953/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9693 - acc: 0.5710 - val_loss: 13.1145 - val_acc: 0.1300\n",
      "Epoch 954/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9707 - acc: 0.5610 - val_loss: 13.1267 - val_acc: 0.1100\n",
      "Epoch 955/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9716 - acc: 0.5650 - val_loss: 13.1430 - val_acc: 0.1200\n",
      "Epoch 956/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 10.9726 - acc: 0.5470 - val_loss: 13.1305 - val_acc: 0.0900\n",
      "Epoch 957/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9714 - acc: 0.5490 - val_loss: 13.1278 - val_acc: 0.1200\n",
      "Epoch 958/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9700 - acc: 0.5700 - val_loss: 13.1322 - val_acc: 0.1100\n",
      "Epoch 959/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9664 - acc: 0.5520 - val_loss: 13.1282 - val_acc: 0.1200\n",
      "Epoch 960/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9687 - acc: 0.5620 - val_loss: 13.1389 - val_acc: 0.1100\n",
      "Epoch 961/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9665 - acc: 0.5730 - val_loss: 13.1247 - val_acc: 0.1300\n",
      "Epoch 962/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9690 - acc: 0.5720 - val_loss: 13.1335 - val_acc: 0.1300\n",
      "Epoch 963/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9690 - acc: 0.5480 - val_loss: 13.1364 - val_acc: 0.1100\n",
      "Epoch 964/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9680 - acc: 0.5570 - val_loss: 13.1389 - val_acc: 0.1100\n",
      "Epoch 965/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 10.9680 - acc: 0.5540 - val_loss: 13.1057 - val_acc: 0.1000\n",
      "Epoch 966/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9691 - acc: 0.5470 - val_loss: 13.1305 - val_acc: 0.1400\n",
      "Epoch 967/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9681 - acc: 0.5400 - val_loss: 13.1499 - val_acc: 0.1000\n",
      "Epoch 968/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9699 - acc: 0.5350 - val_loss: 13.1294 - val_acc: 0.1300\n",
      "Epoch 969/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9691 - acc: 0.5680 - val_loss: 13.1362 - val_acc: 0.1100\n",
      "Epoch 970/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9707 - acc: 0.5570 - val_loss: 13.1336 - val_acc: 0.1100\n",
      "Epoch 971/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9687 - acc: 0.5580 - val_loss: 13.1387 - val_acc: 0.1200\n",
      "Epoch 972/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 10.9672 - acc: 0.5780 - val_loss: 13.1279 - val_acc: 0.1000\n",
      "Epoch 973/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 10.9659 - acc: 0.5670 - val_loss: 13.1555 - val_acc: 0.1300\n",
      "Epoch 974/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9668 - acc: 0.5430 - val_loss: 13.1517 - val_acc: 0.1300\n",
      "Epoch 975/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9665 - acc: 0.5750 - val_loss: 13.1382 - val_acc: 0.1300\n",
      "Epoch 976/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9691 - acc: 0.5480 - val_loss: 13.1478 - val_acc: 0.1100\n",
      "Epoch 977/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9699 - acc: 0.5600 - val_loss: 13.1384 - val_acc: 0.1200\n",
      "Epoch 978/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9676 - acc: 0.5530 - val_loss: 13.1625 - val_acc: 0.1100\n",
      "Epoch 979/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9665 - acc: 0.5630 - val_loss: 13.1512 - val_acc: 0.1300\n",
      "Epoch 980/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9681 - acc: 0.5320 - val_loss: 13.1683 - val_acc: 0.1200\n",
      "Epoch 981/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9664 - acc: 0.5660 - val_loss: 13.1460 - val_acc: 0.1200\n",
      "Epoch 982/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9670 - acc: 0.5540 - val_loss: 13.1648 - val_acc: 0.1300\n",
      "Epoch 983/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9677 - acc: 0.5500 - val_loss: 13.1640 - val_acc: 0.1100\n",
      "Epoch 984/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9656 - acc: 0.5650 - val_loss: 13.1671 - val_acc: 0.1200\n",
      "Epoch 985/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9657 - acc: 0.5530 - val_loss: 13.1580 - val_acc: 0.1300\n",
      "Epoch 986/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9668 - acc: 0.5520 - val_loss: 13.1562 - val_acc: 0.1100\n",
      "Epoch 987/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9640 - acc: 0.5670 - val_loss: 13.1498 - val_acc: 0.1100\n",
      "Epoch 988/1000\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 10.9655 - acc: 0.5800 - val_loss: 13.1692 - val_acc: 0.1200\n",
      "Epoch 989/1000\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 10.9662 - acc: 0.5610 - val_loss: 13.1577 - val_acc: 0.1100\n",
      "Epoch 990/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9690 - acc: 0.5620 - val_loss: 13.1598 - val_acc: 0.1100\n",
      "Epoch 991/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9652 - acc: 0.5580 - val_loss: 13.1788 - val_acc: 0.1100\n",
      "Epoch 992/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9640 - acc: 0.5630 - val_loss: 13.1773 - val_acc: 0.1200\n",
      "Epoch 993/1000\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 10.9659 - acc: 0.5640 - val_loss: 13.1357 - val_acc: 0.1400\n",
      "Epoch 994/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9666 - acc: 0.5610 - val_loss: 13.1768 - val_acc: 0.1200\n",
      "Epoch 995/1000\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 10.9647 - acc: 0.5530 - val_loss: 13.1961 - val_acc: 0.1000\n",
      "Epoch 996/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9645 - acc: 0.5530 - val_loss: 13.1694 - val_acc: 0.1100\n",
      "Epoch 997/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9628 - acc: 0.5540 - val_loss: 13.1872 - val_acc: 0.1200\n",
      "Epoch 998/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9633 - acc: 0.5620 - val_loss: 13.1799 - val_acc: 0.1100\n",
      "Epoch 999/1000\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 10.9648 - acc: 0.5610 - val_loss: 13.1659 - val_acc: 0.1200\n",
      "Epoch 1000/1000\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 10.9644 - acc: 0.5590 - val_loss: 13.1662 - val_acc: 0.1000\n",
      "<tensorflow.python.keras.callbacks.History object at 0x1822a2c9d0>\n"
     ]
    }
   ],
   "source": [
    "#fitting some random data\n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))\n",
    "\n",
    "val_data = np.random.random((100, 32))\n",
    "val_labels = np.random.random((100, 10))\n",
    "\n",
    "History = model.fit(data, labels, epochs=1000, batch_size=32,\n",
    "          validation_data=(val_data, val_labels))\n",
    "print(History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0IUQokBIQBdCiAIhAV0IIQrE/w+075iD4sJTugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182858cd90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for key, val in History.history.iteritems():\n",
    "    plt.plot(val, label=key)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
