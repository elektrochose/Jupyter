{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battery of Tests for Models\n",
    "All the work done previously is assumed to be finished. From here, we work from the folder 'Models/Winners/', which is where 1 copy of a model for each dataset lives. That model is the best performing model we were able to come up with before. Here, we will look at the following:    \n",
    "\n",
    "- Learning curve for test sessions, original and one created by model  \n",
    "- Calculate choice decoding accuracy - remember scores are a lower bound as of now  \n",
    "- Plot model grid  \n",
    "- Calculate similarity scores between datasets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading modules... done\n"
     ]
    }
   ],
   "source": [
    "#BOILERPLATE _______________________\n",
    "#MODULES ______________________\n",
    "print 'loading modules...',\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import operator\n",
    "import pysftp\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "\n",
    "import Visualize as V\n",
    "from behavioral_performance.tools import SequenceClass as SC\n",
    "\n",
    "ROOT = os.environ['HOME'] + '/python/'\n",
    "idx = pd.IndexSlice\n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Grid\n",
    "After evaluating and finding the best performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_scores = pd.DataFrame(np.zeros([len(fileNames),len(fileNames)]),\n",
    "                      index = fileNames,\n",
    "                      columns = fileNames) \n",
    "\n",
    "for index, fileName in enumerate(fileNames):\n",
    "    \n",
    "    sequence_path = '/Users/pablomartin/python/' + \\\n",
    "                    'DATA_structures/RNN_sequences/OneHotBinaryMinimal/' + \\\n",
    "                    dataPrep + '/' + fileName\n",
    "    seqs = pickle.load(open(sequence_path, 'rb'))       \n",
    "\n",
    "    \n",
    "\n",
    "    for cindex, compFileName in enumerate(fileNames):\n",
    "        model_target = '/Users/pablomartin/python/Models/Winners/' + compFileName[:-2] + '.hdf5'\n",
    "        model = load_model(model_target)        \n",
    "        if fileName == compFileName:\n",
    "            loss, acc = model.evaluate(x = seqs.X_test, y = seqs.y_test)\n",
    "            grid_scores.loc[compFileName, fileName] = acc\n",
    "        else:\n",
    "            X = np.concatenate([seqs.X_train, seqs.X_validate, seqs.X_test])\n",
    "            y = np.concatenate([seqs.y_train, seqs.y_validate, seqs.y_test])\n",
    "            loss, acc = model.evaluate(x = X, y = y)\n",
    "            grid_scores.loc[compFileName, fileName] = acc\n",
    "            \n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function will take a Keras model and a session dataframe. It will take the\n",
    "session and prepare it for the model to use, and spit out an accuracy value.\n",
    "'''\n",
    "\n",
    "ROOT = '/Users/pablomartin/python/'\n",
    "model_dirs = ROOT + 'Models/Winners/'\n",
    "session_dirs = ROOT + 'DATA_structures/TbyT/'\n",
    "seq_dirs = ROOT + 'DATA_structures/RNN_sequences/OneHotBinaryMinimal/'\n",
    "seq_length_d = {1: 'Last', 30: 'Med', 200: 'Full'}\n",
    "sets_dict = {'X_train' : 'y_train',\n",
    "             'X_validate' : 'y_validate',\n",
    "             'X_test' : 'y_test'}\n",
    "\n",
    "\n",
    "def RNN_session_pred(session, model):\n",
    "    noTrials = len(session)\n",
    "    batch_size, seq_length, feature_dim = model.input_shape\n",
    "    #checks whether model predicted choice only, not reward\n",
    "    ch_red = lambda x,y: (x>1) == (y>1)\n",
    "    #right now will only work with OneHotBinaryMinimal preparation\n",
    "    combined = session['reward',0] + session['choice',0] * 2\n",
    "    combined = combined.append(pd.Series([0,1,2,3]))\n",
    "    combined = pd.get_dummies(combined)\n",
    "    combined = combined.drop([0,1,2,3])\n",
    "\n",
    "    sanity_check = 0\n",
    "    exact_match, choice_match = 0,0\n",
    "    info = {'choice' : [], 'prediction' : [], 'match' : []}\n",
    "    x_seqs = []\n",
    "    y_seqs = []\n",
    "    for trial in range(1, noTrials - 1):\n",
    "        if trial < seq_length:\n",
    "            x = np.concatenate([np.zeros((seq_length - trial,4)),\n",
    "                                combined.iloc[:trial].values], axis=0)\n",
    "        else:\n",
    "            x = combined.iloc[trial - 30: trial].values\n",
    "\n",
    "\n",
    "        assert x.shape[0] == seq_length\n",
    "        assert x.shape[1] == feature_dim\n",
    "        if trial > 1:\n",
    "            sanity_check += (np.argmax(x[-1]) == y)\n",
    "        x_seqs.append(x)\n",
    "        x = np.reshape(x, (1, seq_length, feature_dim))\n",
    "\n",
    "        y_seqs.append(combined.iloc[trial])\n",
    "        y = np.argmax(combined.iloc[trial])\n",
    "        pred_x = np.argmax(model.predict(x))\n",
    "        exact_match += (y == pred_x)\n",
    "        choice_match += ch_red(pred_x, y)\n",
    "\n",
    "        info['choice'].append(y)\n",
    "        info['prediction'].append(pred_x)\n",
    "        info['match'].append(ch_red(pred_x, y))\n",
    "    x_seqs = np.asarray(x_seqs)\n",
    "    y_seqs = np.asarray(y_seqs)\n",
    "    values = model.evaluate(x = x_seqs, y = y_seqs, verbose = 0)\n",
    "\n",
    "    exact_match_score = np.float(exact_match) / (noTrials - 2)\n",
    "    choice_match_score = np.float(choice_match) / (noTrials - 2)\n",
    "    sanity_check_score  = np.float(sanity_check) / (noTrials -3)\n",
    "    evaluate_score = values[1]\n",
    "    if sanity_check_score != 1:\n",
    "        print 'sanity check failed: %.2f - should be 1.00' %(sanity_check_score)\n",
    "    return [exact_match_score, choice_match_score, info, evaluate_score]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for datasetIndex, fileName in enumerate(fileNames):\n",
    "    print '*' * 80\n",
    "    print 'working on dataset: %s' %(fileName)\n",
    "    df = pickle.load(open(session_dirs + fileName, 'rb'))\n",
    "    sessions = df.groupby(axis = 0, level = 'session')\n",
    "\n",
    "    print 'loading model ...',\n",
    "    model = load_model(model_dirs + fileName[:-2] + '.hdf5')\n",
    "    print 'done'\n",
    "\n",
    "    print 'loading sequences ...',\n",
    "    dataPrep = seq_length_d[model.input_shape[1]] + '/'\n",
    "    seqs = pickle.load(open(seq_dirs + dataPrep + fileName, 'rb'))\n",
    "    print 'done'\n",
    "\n",
    "    #determining what session belongs to what group\n",
    "    print 'determining what set each session belonged to ...',\n",
    "    TRAIN, VALIDATE, TEST = seqs.train_validate_test_split_by_session(df)\n",
    "    TRAIN_SESSIONS = \\\n",
    "            [w for w, s in TRAIN.groupby(axis = 0, level = 'session')]\n",
    "    VALIDATE_SESSIONS = \\\n",
    "            [w for w, s in VALIDATE.groupby(axis = 0, level = 'session')]\n",
    "    TEST_SESSIONS = \\\n",
    "            [w for w, s in TEST.groupby(axis = 0, level = 'session')]\n",
    "    print 'done'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"X_train: {}, {}\".format(seqs.X_train.dtype, seqs.X_train.shape))\n",
    "    print(\"y_train: {}, {}\".format(seqs.y_train.dtype, seqs.y_train.shape))\n",
    "\n",
    "\n",
    "    print 'Sequence scores using model.evaluate ...',\n",
    "    results_train = \\\n",
    "        model.evaluate(x = seqs.X_train, y = seqs.y_train, verbose = 0)\n",
    "    print 'training score : %.2f' %(results_train[1])\n",
    "\n",
    "    print 'Sequence scores using model.predict ...',\n",
    "    preds = np.argmax(model.predict(seqs.X_train), axis = 1)\n",
    "    actual = np.argmax(seqs.y_train, axis=1)\n",
    "    manual_score = np.float(np.sum(preds == actual)) / len(seqs.X_train)\n",
    "    print 'training score : %.2f' %(manual_score)\n",
    "\n",
    "    print 'changing dtype of y_train'\n",
    "    seqs.y_train = seqs.y_train.astype(np.float64)\n",
    "    print(\"X_train: {}, {}\".format(seqs.X_train.dtype, seqs.X_train.shape))\n",
    "    print(\"y_train: {}, {}\".format(seqs.y_train.dtype, seqs.y_train.shape))\n",
    "\n",
    "\n",
    "    print 'Sequence scores using model.evaluate ...',\n",
    "    results_train = \\\n",
    "        model.evaluate(x = seqs.X_train, y = seqs.y_train, verbose = 0)\n",
    "\n",
    "    print 'training score : %.2f' %(results_train[1])\n",
    "    print model.metrics_names\n",
    "    print 'Sequence scores using model.predict ...',\n",
    "    preds = np.argmax(model.predict(seqs.X_train), axis = 1)\n",
    "    actual = np.argmax(seqs.y_train, axis=1)\n",
    "    manual_score = np.float(np.sum(preds == actual)) / len(seqs.X_train)\n",
    "    print 'training score : %.2f' %(manual_score)\n",
    "\n",
    "\n",
    "    match_by_hand = {}\n",
    "\n",
    "    for setIndex, set in enumerate(sets_dict.keys()):\n",
    "        tmp_match_by_hand = 0\n",
    "        for t in xrange(len(seqs.__dict__[set])):\n",
    "            pred = np.argmax(model.predict(\n",
    "                    np.reshape(seqs.__dict__[set][t], (1,model.input_shape[1],4))))\n",
    "            y_actual = np.argmax(seqs.__dict__[sets_dict[set]][t])\n",
    "            tmp_match_by_hand += (pred == y_actual)\n",
    "        match_by_hand[set] = \\\n",
    "                np.float(tmp_match_by_hand) / len(seqs.__dict__[set])\n",
    "\n",
    "\n",
    "\n",
    "    results_validate = \\\n",
    "        model.evaluate(x = seqs.X_validate, y = seqs.y_validate, verbose = 0)\n",
    "    results_test = \\\n",
    "        model.evaluate(x = seqs.X_test, y = seqs.y_test, verbose = 0)\n",
    "    print 'done'\n",
    "\n",
    "    print 'evaluating score session by session ...',\n",
    "    session_dict = {}\n",
    "    for label, session in sessions:\n",
    "        [exact_match_score, choice_match_score, info, evaluate_score] = \\\n",
    "                                            RNN_session_pred(session, model)\n",
    "\n",
    "        try:\n",
    "            tmp = TRAIN_SESSIONS.index(label)\n",
    "            membership = 'train'\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            tmp = VALIDATE_SESSIONS.index(label)\n",
    "            membership = 'validate'\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            tmp = TEST_SESSIONS.index(label)\n",
    "            membership = 'test'\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "\n",
    "        session_dict[label] = \\\n",
    "        (exact_match_score, evaluate_score, membership, choice_match_score)\n",
    "    print 'done'\n",
    "\n",
    "\n",
    "    print '\\n'\n",
    "    print 'training score : %.2f' %(results_train[1])\n",
    "    print 'validation score : %.2f' %(results_validate[1])\n",
    "    print 'testing score : %.2f' %(results_test[1])\n",
    "    print 'scores by hand: %s' %(match_by_hand)\n",
    "    print '\\n'\n",
    "    for sess_label in session_dict.keys():\n",
    "        print 'score for session %s : %.2f - from %s - evaluate: %.2f, choice: %.2f'\\\n",
    "                                            %(sess_label,\n",
    "                                            session_dict[sess_label][0],\n",
    "                                            session_dict[sess_label][2],\n",
    "                                            session_dict[sess_label][1],\n",
    "                                            session_dict[sess_label][3])\n",
    "    print '\\n'\n",
    "    print '*' * 80\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
